{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 📘 Pandas Tutorial (Zero to Hero)\n",
        "\n",
        "## 1. Introduction to Pandas\n",
        "\n",
        "* What is Pandas? (built on NumPy, for data manipulation & analysis)\n",
        "* Why Pandas vs Python lists/dictionaries/NumPy?\n",
        "* Installing Pandas\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Pandas Basics\n",
        "\n",
        "* `Series` (1D labeled array)\n",
        "* `DataFrame` (2D labeled data structure)\n",
        "* Creating Series & DataFrames (from lists, dicts, NumPy arrays, CSV, Excel)\n",
        "\n",
        "---\n",
        "\n",
        "## 3. DataFrame Attributes\n",
        "\n",
        "* `.shape`, `.ndim`, `.dtypes`, `.columns`, `.index`, `.values`\n",
        "* Getting quick insights: `.head()`, `.tail()`, `.info()`, `.describe()`\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Indexing & Selection\n",
        "\n",
        "* Column selection (`df['col']`, `df.col`)\n",
        "* Row selection (`.loc`, `.iloc`)\n",
        "* Slicing DataFrames\n",
        "* Boolean indexing & filtering\n",
        "* Setting index & resetting index\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Data Cleaning\n",
        "\n",
        "* Handling missing values (`isna`, `fillna`, `dropna`)\n",
        "* Handling duplicates (`duplicated`, `drop_duplicates`)\n",
        "* Renaming columns\n",
        "* Changing data types (`astype`)\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Operations\n",
        "\n",
        "* Arithmetic operations\n",
        "* String operations (`.str`)\n",
        "* Apply functions (`apply`, `map`, `applymap`)\n",
        "* Sorting (`sort_values`, `sort_index`)\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Grouping & Aggregation\n",
        "\n",
        "* `groupby` basics\n",
        "* Aggregate functions (`sum`, `mean`, `count`, `agg`)\n",
        "* Pivot tables\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Merging, Joining & Concatenation\n",
        "\n",
        "* `concat`, `merge`, `join`\n",
        "* Differences between them\n",
        "* Examples (like SQL joins)\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Working with Dates & Time\n",
        "\n",
        "* `pd.to_datetime`\n",
        "* Extracting year, month, day\n",
        "* Resampling & time-series analysis\n",
        "\n",
        "---\n",
        "\n",
        "## 10. Input/Output with Files\n",
        "\n",
        "* Reading: `read_csv`, `read_excel`, `read_json`, `read_sql`\n",
        "* Writing: `to_csv`, `to_excel`, `to_json`\n",
        "\n",
        "---\n",
        "\n",
        "## 11. Advanced Topics\n",
        "\n",
        "* MultiIndex (hierarchical indexing)\n",
        "* Window functions (rolling, expanding)\n",
        "* Categorical data\n",
        "* Sparse data\n",
        "* Performance tips (vectorization, `.eval`, `.query`)\n",
        "\n",
        "---\n",
        "\n",
        "## 12. Exercises & Projects\n",
        "\n",
        "* Small exercises after each section\n",
        "* Mini-projects at the end:\n",
        "\n",
        "  * Analyzing a CSV dataset (e.g., Titanic dataset, sales dataset)\n",
        "  * Cleaning messy data (missing values, duplicates, etc.)\n",
        "  * Simple exploratory data analysis (EDA)\n",
        "\n",
        "---\n",
        "\n",
        "⚡ **Extra Tips & Tricks**\n",
        "\n",
        "* Use `.copy()` to avoid SettingWithCopyWarning\n",
        "* Prefer `loc` over chained indexing\n",
        "* Use `df.query()` for cleaner filtering\n",
        "* Use `.astype('category')` for memory optimization"
      ],
      "metadata": {
        "id": "T46ry0OM5BIi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMdmdNh64ceZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📘 1. Introduction to Pandas\n",
        "\n",
        "### 🔹 What is Pandas?\n",
        "\n",
        "* **Pandas** is an open-source **Python library** used for **data manipulation and analysis**.\n",
        "* It is **built on top of NumPy**, which means it uses NumPy arrays under the hood for fast computations.\n",
        "* It provides two main data structures:\n",
        "\n",
        "  * **Series** → 1D labeled data (like a column in Excel)\n",
        "  * **DataFrame** → 2D labeled data (like a full Excel sheet or SQL table)\n",
        "\n",
        "👉 In short: Pandas is the **go-to library** for working with structured (tabular) data in Python.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 Why Pandas vs. Python lists/dictionaries/NumPy?\n",
        "\n",
        "* **Python lists/dictionaries**\n",
        "\n",
        "  * Good for small tasks, but become slow & messy with large datasets.\n",
        "  * No built-in tools for filtering, grouping, or joining data.\n",
        "\n",
        "* **NumPy arrays**\n",
        "\n",
        "  * Great for numerical data & fast calculations.\n",
        "  * But lacks **labels** (column names, row indices).\n",
        "  * Harder to handle **heterogeneous data** (numbers + text).\n",
        "\n",
        "* **Pandas**\n",
        "  ✅ Combines the **speed of NumPy** with the **flexibility of labels**.\n",
        "  ✅ Offers **built-in methods** for filtering, cleaning, grouping, merging, reshaping.\n",
        "  ✅ Works seamlessly with **CSV, Excel, SQL, JSON, and more**.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 Installing Pandas\n",
        "\n",
        "Pandas doesn’t come pre-installed with Python. Install it using:\n",
        "\n",
        "```bash\n",
        "pip install pandas\n",
        "```\n",
        "\n",
        "Or if you’re using **Anaconda** (recommended for data science):\n",
        "\n",
        "```bash\n",
        "conda install pandas\n",
        "```\n",
        "\n",
        "Verify the installation inside Python:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "print(pd.__version__)\n",
        "```"
      ],
      "metadata": {
        "id": "BycPIMC75Lkk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PAFv8niC5lZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# 📘 2. Pandas Basics\n",
        "\n",
        "Pandas has two main data structures:\n",
        "\n",
        "1. **Series** → One-dimensional (1D) labeled array.\n",
        "2. **DataFrame** → Two-dimensional (2D) labeled data structure (rows + columns).\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 2.1 Series (1D Labeled Array)\n",
        "\n",
        "* Think of a **Series** like a single column in Excel or a single column in a SQL table.\n",
        "* It has two parts:\n",
        "\n",
        "  * **Values** → actual data (numbers, strings, etc.)\n",
        "  * **Index** → labels for each value\n",
        "\n",
        "### Example 1: Creating a Series from a list\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Create a Series from a list\n",
        "data = [10, 20, 30, 40]\n",
        "s = pd.Series(data)\n",
        "print(s)\n",
        "```\n",
        "\n",
        "**Output:**\n",
        "\n",
        "```\n",
        "0    10\n",
        "1    20\n",
        "2    30\n",
        "3    40\n",
        "dtype: int64\n",
        "```\n",
        "\n",
        "👉 By default, Pandas assigns **integer indices (0,1,2,3)**.\n",
        "\n",
        "---\n",
        "\n",
        "### Example 2: Custom Index\n",
        "\n",
        "```python\n",
        "s = pd.Series([10, 20, 30, 40], index=[\"A\", \"B\", \"C\", \"D\"])\n",
        "print(s)\n",
        "```\n",
        "\n",
        "**Output:**\n",
        "\n",
        "```\n",
        "A    10\n",
        "B    20\n",
        "C    30\n",
        "D    40\n",
        "dtype: int64\n",
        "```\n",
        "\n",
        "👉 Now, instead of default numbers, we gave labels.\n",
        "\n",
        "---\n",
        "\n",
        "### Example 3: From Dictionary\n",
        "\n",
        "```python\n",
        "data = {\"Apple\": 3, \"Banana\": 5, \"Orange\": 2}\n",
        "s = pd.Series(data)\n",
        "print(s)\n",
        "```\n",
        "\n",
        "**Output:**\n",
        "\n",
        "```\n",
        "Apple     3\n",
        "Banana    5\n",
        "Orange    2\n",
        "dtype: int64\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 2.2 DataFrame (2D Labeled Data Structure)\n",
        "\n",
        "* A **DataFrame** is like an Excel sheet or SQL table → rows + columns.\n",
        "* Each column is a **Series**, and the whole table is a **DataFrame**.\n",
        "\n",
        "---\n",
        "\n",
        "### Example 1: From Dictionary of Lists\n",
        "\n",
        "```python\n",
        "data = {\n",
        "    \"Name\": [\"Ali\", \"Sara\", \"John\"],\n",
        "    \"Age\": [25, 30, 28],\n",
        "    \"City\": [\"Cairo\", \"Paris\", \"London\"]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(df)\n",
        "```\n",
        "\n",
        "**Output:**\n",
        "\n",
        "```\n",
        "   Name  Age    City\n",
        "0   Ali   25   Cairo\n",
        "1  Sara   30   Paris\n",
        "2  John   28  London\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Example 2: From List of Dictionaries\n",
        "\n",
        "```python\n",
        "data = [\n",
        "    {\"Name\": \"Ali\", \"Age\": 25, \"City\": \"Cairo\"},\n",
        "    {\"Name\": \"Sara\", \"Age\": 30, \"City\": \"Paris\"},\n",
        "    {\"Name\": \"John\", \"Age\": 28, \"City\": \"London\"}\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(df)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Example 3: From NumPy Array\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "arr = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "df = pd.DataFrame(arr, columns=[\"A\", \"B\", \"C\"])\n",
        "print(df)\n",
        "```\n",
        "\n",
        "**Output:**\n",
        "\n",
        "```\n",
        "   A  B  C\n",
        "0  1  2  3\n",
        "1  4  5  6\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Example 4: From CSV/Excel File\n",
        "\n",
        "```python\n",
        "# From CSV\n",
        "df_csv = pd.read_csv(\"data.csv\")\n",
        "\n",
        "# From Excel\n",
        "df_excel = pd.read_excel(\"data.xlsx\")\n",
        "\n",
        "print(df_csv.head())   # First 5 rows\n",
        "print(df_excel.head())\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 📝 Exercises\n",
        "\n",
        "### **Exercise 1**\n",
        "\n",
        "Create a **Series** that represents the number of books each student has:\n",
        "\n",
        "* Ali: 3, Sara: 7, John: 5, Noor: 9\n",
        "\n",
        "---\n",
        "\n",
        "### **Exercise 2**\n",
        "\n",
        "Create a **DataFrame** for the following student data:\n",
        "\n",
        "| Name | Age | Grade |\n",
        "| ---- | --- | ----- |\n",
        "| Ali  | 20  | A     |\n",
        "| Sara | 22  | B     |\n",
        "| John | 21  | A     |\n",
        "| Noor | 23  | C     |\n",
        "\n",
        "---\n",
        "\n",
        "### **Exercise 3**\n",
        "\n",
        "Use **NumPy** to create a 2D array of shape (3,3), then convert it into a Pandas **DataFrame** with column names `X, Y, Z`.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Solutions\n",
        "\n",
        "### Solution 1\n",
        "\n",
        "```python\n",
        "books = {\"Ali\": 3, \"Sara\": 7, \"John\": 5, \"Noor\": 9}\n",
        "s = pd.Series(books)\n",
        "print(s)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Solution 2\n",
        "\n",
        "```python\n",
        "data = {\n",
        "    \"Name\": [\"Ali\", \"Sara\", \"John\", \"Noor\"],\n",
        "    \"Age\": [20, 22, 21, 23],\n",
        "    \"Grade\": [\"A\", \"B\", \"A\", \"C\"]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(df)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Solution 3\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "arr = np.array([[1,2,3], [4,5,6], [7,8,9]])\n",
        "df = pd.DataFrame(arr, columns=[\"X\", \"Y\", \"Z\"])\n",
        "print(df)\n",
        "```"
      ],
      "metadata": {
        "id": "_pHUEq_d5lx7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2_wF0361Ntxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) DataFrame Attributes — Deep Dive\n",
        "\n",
        "We’ll use this small dataset throughout:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"Name\": [\"Ali\", \"Sara\", \"John\", \"Noor\", \"Mona\", \"Omar\"],\n",
        "    \"Dept\": [\"IT\", \"HR\", \"Finance\", \"IT\", \"HR\", \"Finance\"],\n",
        "    \"Age\": [20, 22, 21, 23, 29, 35],\n",
        "    \"Salary\": [5000, 4500, 6000, 5500, 5200, 7200],\n",
        "    \"Joined\": pd.to_datetime([\"2022-01-10\",\"2021-05-03\",\"2020-07-19\",\"2022-02-01\",\"2019-09-30\",\"2018-11-11\"]),\n",
        "    \"Remote\": [True, False, None, True, False, True]\n",
        "})\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## A) Shape & Dimensionality\n",
        "\n",
        "### `.shape` → `(rows, columns)`\n",
        "\n",
        "```python\n",
        "df.shape       # e.g., (6, 6)\n",
        "```\n",
        "\n",
        "### `.ndim` → number of dimensions\n",
        "\n",
        "* `2` for DataFrame; `1` for Series.\n",
        "\n",
        "```python\n",
        "df.ndim        # 2\n",
        "```\n",
        "\n",
        "### Related helpers\n",
        "\n",
        "```python\n",
        "len(df)        # rows\n",
        "df.size        # rows * columns (total cells)\n",
        "df.axes        # [row_index, column_index]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## B) Dtypes (Data Types)\n",
        "\n",
        "### `.dtypes` — each column’s dtype\n",
        "\n",
        "```python\n",
        "df.dtypes\n",
        "```\n",
        "\n",
        "Typical dtypes: `int64`, `float64`, `bool`, `object` (strings/mixed), `datetime64[ns]`, `category`, and **nullable** types like `Int64`, `boolean`, `string`.\n",
        "\n",
        "### Pick columns by dtype\n",
        "\n",
        "```python\n",
        "df.select_dtypes(include=\"number\")\n",
        "df.select_dtypes(exclude=[\"object\",\"datetime\"])\n",
        "```\n",
        "\n",
        "### Convert dtypes (smartly)\n",
        "\n",
        "```python\n",
        "df2 = df.convert_dtypes()  # uses pandas’ nullable types where possible\n",
        "```\n",
        "\n",
        "### Force a dtype\n",
        "\n",
        "```python\n",
        "df[\"Dept\"] = df[\"Dept\"].astype(\"category\")\n",
        "```\n",
        "\n",
        "> Tip: `object` often means “strings” but can hide mixed types; converting to `category` or `string` saves memory and clarifies intent.\n",
        "\n",
        "---\n",
        "\n",
        "## C) Columns & Index\n",
        "\n",
        "### `.columns` — column labels (an `Index`)\n",
        "\n",
        "```python\n",
        "df.columns\n",
        "df.columns.tolist()\n",
        "```\n",
        "\n",
        "Rename or set:\n",
        "\n",
        "```python\n",
        "df = df.rename(columns={\"Dept\": \"Department\"})\n",
        "df.columns = [c.lower() for c in df.columns]   # overwrite all at once\n",
        "```\n",
        "\n",
        "### `.index` — row labels\n",
        "\n",
        "```python\n",
        "df.index           # RangeIndex(...) by default\n",
        "df.set_index(\"name\", inplace=False)   # use a column as index (returns a copy)\n",
        "df.reset_index(drop=True)             # back to RangeIndex\n",
        "```\n",
        "\n",
        "Common index types: `RangeIndex`, `Index` (generic), `DatetimeIndex`, `MultiIndex`.\n",
        "\n",
        "> Tip: Give your index a meaning (e.g., an ID or date). It improves merging, slicing, and time-series operations.\n",
        "\n",
        "---\n",
        "\n",
        "## D) Values as Arrays\n",
        "\n",
        "### `.values` (NumPy ndarray)\n",
        "\n",
        "Returns the underlying array. Mixed dtypes → `object` array.\n",
        "\n",
        "```python\n",
        "arr = df.values\n",
        "```\n",
        "\n",
        "### `.to_numpy(dtype=..., copy=...)` (preferred)\n",
        "\n",
        "```python\n",
        "arr = df.to_numpy()           # safe, respects extension dtypes\n",
        "arr64 = df.select_dtypes(\"number\").to_numpy(dtype=\"float64\")\n",
        "```\n",
        "\n",
        "> Prefer `to_numpy()` for clarity and control over dtype/copy.\n",
        "> `.values` can surprise you with `object` dtype when columns are mixed.\n",
        "\n",
        "---\n",
        "\n",
        "## E) Quick Peek / Profiling\n",
        "\n",
        "### `.head(n)` / `.tail(n)` — first/last rows\n",
        "\n",
        "```python\n",
        "df.head(3)\n",
        "df.tail(2)\n",
        "```\n",
        "\n",
        "### `.sample(n=..., frac=...)` — quick random check\n",
        "\n",
        "```python\n",
        "df.sample(2, random_state=0)\n",
        "```\n",
        "\n",
        "### `.info()` — schema summary, nulls, memory\n",
        "\n",
        "```python\n",
        "df.info()\n",
        "df.info(show_counts=True)         # show non-null counts explicitly\n",
        "df.info(memory_usage=\"deep\")      # more accurate memory sizes\n",
        "```\n",
        "\n",
        "### `.describe()` — stats summary\n",
        "\n",
        "```python\n",
        "df.describe()                     # numeric columns only\n",
        "df.describe(include=\"all\")        # all columns (object/category counts, top, freq)\n",
        "df.describe(percentiles=[.05,.5,.95])\n",
        "```\n",
        "\n",
        "### Fast missingness snapshot\n",
        "\n",
        "```python\n",
        "df.isna().sum()                   # NA count per column\n",
        "df.isna().mean()                  # NA ratio per column\n",
        "```\n",
        "\n",
        "### Other handy peeks\n",
        "\n",
        "```python\n",
        "df.nunique()                      # distinct values per column\n",
        "df[\"dept\"].value_counts(dropna=False)  # frequency table for a column\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## F) Other Handy Attributes\n",
        "\n",
        "```python\n",
        "df.T              # transpose (swap rows/cols)\n",
        "df.empty          # True if df has 0 elements (rows*cols == 0)\n",
        "df.memory_usage(deep=True)      # per-column memory usage\n",
        "df.columns.name = \"fields\"      # name the columns index\n",
        "df.index.name = \"row_id\"        # name the row index\n",
        "df.attrs[\"source\"] = \"HR export\"  # attach arbitrary metadata\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# 📝 Exercises (at least 10 — here are 14)\n",
        "\n",
        "Assume `df` is the DataFrame defined at the top unless stated otherwise.\n",
        "\n",
        "1. **Shapes & Sizes**\n",
        "\n",
        "   * Print: number of rows, number of columns, total cells (three separate lines).\n",
        "\n",
        "2. **Dimensionality Check**\n",
        "\n",
        "   * Verify that `df` is 2D and that `df[\"age\"]` is 1D using `.ndim`.\n",
        "\n",
        "3. **Index Naming**\n",
        "\n",
        "   * Set the index to the `name` column (lowercase), name the index `\"employee\"`, then reset it back.\n",
        "\n",
        "4. **Column Cleanup**\n",
        "\n",
        "   * Lowercase all column names and replace spaces with underscores.\n",
        "\n",
        "5. **Dtype Audit**\n",
        "\n",
        "   * Show a table of: column name, dtype, number of nulls, number of unique values.\n",
        "\n",
        "6. **Select by Dtype**\n",
        "\n",
        "   * Get a DataFrame containing only numeric columns. Then compute `.describe()` on it with percentiles at 5%, 50%, 95%.\n",
        "\n",
        "7. **Memory Matters**\n",
        "\n",
        "   * Show memory usage per column with `deep=True`.\n",
        "   * Convert the `dept` column to `category`. Show memory usage again.\n",
        "   * Report how many bytes you saved.\n",
        "\n",
        "8. **Smart Type Conversion**\n",
        "\n",
        "   * Run `convert_dtypes()` and print the dtypes before vs after. Which columns changed and how?\n",
        "\n",
        "9. **Missingness Snapshot**\n",
        "\n",
        "   * Compute a two-line summary:\n",
        "\n",
        "     * (a) NA counts per column\n",
        "     * (b) NA ratios (0–1) per column, sorted descending.\n",
        "\n",
        "10. **Describe Everything**\n",
        "\n",
        "    * Run `df.describe(include=\"all\")` and interpret: which column has the most frequent value shown by `top`/`freq`?\n",
        "\n",
        "11. **Quick Peek Trio**\n",
        "\n",
        "    * Show the first 2 rows, last 2 rows, and a random sample of 2 rows (fixed `random_state` for reproducibility).\n",
        "\n",
        "12. **Values vs to\\_numpy**\n",
        "\n",
        "    * Extract all numeric data as a NumPy array with dtype `float64` using **one** line of code (no intermediate variables).\n",
        "    * Explain why `.values` could be risky on mixed-type DataFrames.\n",
        "\n",
        "13. **Index Types**\n",
        "\n",
        "    * Create `df2 = df.set_index(\"joined\")`. What is the index type now? Verify using `type(df2.index)` and `.dtypes`.\n",
        "\n",
        "14. **Profile Helper**\n",
        "\n",
        "    * Build a small function `quick_profile(df)` that returns a DataFrame with columns:\n",
        "      `[\"col\",\"dtype\",\"non_null\",\"nulls\",\"nunique\",\"memory_bytes\"]`\n",
        "      sorted by `memory_bytes` descending.\n",
        "\n",
        "---\n",
        "\n",
        "# ✅ Solutions\n",
        "\n",
        "**1) Shapes & Sizes**\n",
        "\n",
        "```python\n",
        "rows, cols = df.shape\n",
        "print(rows)\n",
        "print(cols)\n",
        "print(df.size)\n",
        "```\n",
        "\n",
        "**2) Dimensionality Check**\n",
        "\n",
        "```python\n",
        "print(df.ndim)          # 2\n",
        "print(df[\"age\"].ndim)   # 1\n",
        "```\n",
        "\n",
        "**3) Index Naming**\n",
        "\n",
        "```python\n",
        "df_ix = df.set_index(\"name\").copy()\n",
        "df_ix.index.name = \"employee\"\n",
        "print(df_ix.index)      # check name\n",
        "df_back = df_ix.reset_index()\n",
        "```\n",
        "\n",
        "**4) Column Cleanup**\n",
        "\n",
        "```python\n",
        "df.columns = [c.lower().replace(\" \", \"_\") for c in df.columns]\n",
        "print(df.columns)\n",
        "```\n",
        "\n",
        "**5) Dtype Audit**\n",
        "\n",
        "```python\n",
        "audit = pd.DataFrame({\n",
        "    \"col\": df.columns,\n",
        "    \"dtype\": df.dtypes.values,\n",
        "    \"nulls\": df.isna().sum().values,\n",
        "    \"nunique\": df.nunique(dropna=False).values\n",
        "})\n",
        "print(audit)\n",
        "```\n",
        "\n",
        "**6) Select by Dtype**\n",
        "\n",
        "```python\n",
        "num_df = df.select_dtypes(include=\"number\")\n",
        "print(num_df.describe(percentiles=[.05,.5,.95]))\n",
        "```\n",
        "\n",
        "**7) Memory Matters**\n",
        "\n",
        "```python\n",
        "before = df.memory_usage(deep=True)\n",
        "df_cat = df.copy()\n",
        "df_cat[\"dept\"] = df_cat[\"dept\"].astype(\"category\")\n",
        "after = df_cat.memory_usage(deep=True)\n",
        "saved = int(before.sum() - after.sum())\n",
        "print(before)\n",
        "print(after)\n",
        "print(\"Bytes saved:\", saved)\n",
        "```\n",
        "\n",
        "**8) Smart Type Conversion**\n",
        "\n",
        "```python\n",
        "print(\"Before:\\n\", df.dtypes)\n",
        "df_conv = df.convert_dtypes()\n",
        "print(\"After:\\n\", df_conv.dtypes)\n",
        "# You’ll likely see 'remote' → boolean, strings → string dtype\n",
        "```\n",
        "\n",
        "**9) Missingness Snapshot**\n",
        "\n",
        "```python\n",
        "na_counts = df.isna().sum()\n",
        "na_ratio = df.isna().mean().sort_values(ascending=False)\n",
        "print(na_counts)\n",
        "print(na_ratio)\n",
        "```\n",
        "\n",
        "**10) Describe Everything**\n",
        "\n",
        "```python\n",
        "desc_all = df.describe(include=\"all\")\n",
        "print(desc_all)\n",
        "# Look at rows 'top' and 'freq' for non-numeric columns (e.g., dept)\n",
        "```\n",
        "\n",
        "**11) Quick Peek Trio**\n",
        "\n",
        "```python\n",
        "print(df.head(2))\n",
        "print(df.tail(2))\n",
        "print(df.sample(2, random_state=42))\n",
        "```\n",
        "\n",
        "**12) Values vs to\\_numpy**\n",
        "\n",
        "```python\n",
        "arr = df.select_dtypes(include=\"number\").to_numpy(dtype=\"float64\")\n",
        "# Why .values can be risky:\n",
        "# On mixed dtypes, .values may become object dtype, losing numeric efficiency and semantics.\n",
        "```\n",
        "\n",
        "**13) Index Types**\n",
        "\n",
        "```python\n",
        "df2 = df.set_index(\"joined\")\n",
        "print(type(df2.index))   # <class 'pandas.core.indexes.datetimes.DatetimeIndex'>\n",
        "print(df2.dtypes)\n",
        "```\n",
        "\n",
        "**14) Profile Helper**\n",
        "\n",
        "```python\n",
        "def quick_profile(x: pd.DataFrame) -> pd.DataFrame:\n",
        "    return pd.DataFrame({\n",
        "        \"col\": x.columns,\n",
        "        \"dtype\": x.dtypes.astype(str).values,\n",
        "        \"non_null\": x.notna().sum().values,\n",
        "        \"nulls\": x.isna().sum().values,\n",
        "        \"nunique\": x.nunique(dropna=False).values,\n",
        "        \"memory_bytes\": x.memory_usage(deep=True, index=False).values\n",
        "    }).sort_values(\"memory_bytes\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(quick_profile(df))\n",
        "```"
      ],
      "metadata": {
        "id": "7AYurS_xNuVT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UcdU2fZ9N1Zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Indexing & Selection in Pandas\n",
        "\n",
        "Indexing & selection is how you **zoom in** on the rows and columns you need in a DataFrame. Think of it like controlling a giant Excel sheet with precise commands.\n",
        "\n",
        "---\n",
        "\n",
        "## 4.1 Column Selection\n",
        "\n",
        "Two main ways:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    \"Name\": [\"Ali\", \"Sara\", \"Omar\", \"Nora\"],\n",
        "    \"Age\": [25, 30, 22, 28],\n",
        "    \"City\": [\"Cairo\", \"Alexandria\", \"Giza\", \"Mansoura\"]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Bracket notation\n",
        "print(df[\"Name\"])   # Always works\n",
        "\n",
        "# Dot notation\n",
        "print(df.Age)       # Works if column has no spaces/special characters\n",
        "```\n",
        "\n",
        "👉 **Tips & Tricks:**\n",
        "\n",
        "* Prefer `df[\"col\"]` for reliability (dot notation may fail if col = \"count\" or \"sum\").\n",
        "* You can select multiple columns at once: `df[[\"Name\", \"City\"]]`.\n",
        "\n",
        "---\n",
        "\n",
        "## 4.2 Row Selection\n",
        "\n",
        "Two main approaches:\n",
        "\n",
        "```python\n",
        "# .loc -> label-based\n",
        "print(df.loc[0])        # Row with label 0\n",
        "print(df.loc[1:2])      # From label 1 to label 2 (inclusive!)\n",
        "\n",
        "# .iloc -> position-based\n",
        "print(df.iloc[0])       # Row at position 0\n",
        "print(df.iloc[1:3])     # Position 1 and 2 (end-exclusive)\n",
        "```\n",
        "\n",
        "👉 **Tips & Tricks:**\n",
        "\n",
        "* `.loc[a:b]` includes the last index, `.iloc[a:b]` excludes it.\n",
        "* Combine row & column selection: `df.loc[0, \"Name\"]`.\n",
        "\n",
        "---\n",
        "\n",
        "## 4.3 Slicing DataFrames\n",
        "\n",
        "You can slice rows like lists, and columns with `.loc`.\n",
        "\n",
        "```python\n",
        "# Slice rows\n",
        "print(df[1:3])   # Rows 1 and 2\n",
        "\n",
        "# Select subset of columns\n",
        "print(df.loc[:, [\"Name\", \"City\"]])\n",
        "\n",
        "# Range of rows + specific columns\n",
        "print(df.loc[1:3, [\"Name\", \"Age\"]])\n",
        "```\n",
        "\n",
        "👉 **Tip:** Use `:` to mean “all” (e.g., `df.loc[:, \"Age\"]` means all rows, only Age column).\n",
        "\n",
        "---\n",
        "\n",
        "## 4.4 Boolean Indexing & Filtering\n",
        "\n",
        "This is where Pandas shines ✨.\n",
        "\n",
        "```python\n",
        "# Rows where Age > 25\n",
        "print(df[df[\"Age\"] > 25])\n",
        "\n",
        "# Multiple conditions\n",
        "print(df[(df[\"Age\"] > 25) & (df[\"City\"] == \"Cairo\")])\n",
        "```\n",
        "\n",
        "👉 **Tips & Tricks:**\n",
        "\n",
        "* Use `&` for AND, `|` for OR, and wrap conditions in parentheses.\n",
        "* Use `.isin()` for matching multiple values: `df[df[\"City\"].isin([\"Cairo\",\"Giza\"])]`.\n",
        "* Use `.str.contains()` for text filters: `df[df[\"City\"].str.contains(\"Cai\")]`.\n",
        "\n",
        "---\n",
        "\n",
        "## 4.5 Setting Index & Resetting Index\n",
        "\n",
        "Sometimes a column is more meaningful as the index.\n",
        "\n",
        "```python\n",
        "# Set index\n",
        "df2 = df.set_index(\"Name\")\n",
        "print(df2)\n",
        "\n",
        "# Select row by index\n",
        "print(df2.loc[\"Omar\"])\n",
        "\n",
        "# Reset to default integer index\n",
        "print(df2.reset_index())\n",
        "```\n",
        "\n",
        "👉 **Tips & Tricks:**\n",
        "\n",
        "* Use `inplace=True` if you don’t want a copy: `df.set_index(\"Name\", inplace=True)`.\n",
        "* After setting index, you can quickly access rows with `df.loc[index_value]`.\n",
        "\n",
        "---\n",
        "\n",
        "# 📝 Exercises\n",
        "\n",
        "Using the same DataFrame `df`:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    \"Name\": [\"Ali\", \"Sara\", \"Omar\", \"Nora\", \"Mona\"],\n",
        "    \"Age\": [25, 30, 22, 28, 35],\n",
        "    \"City\": [\"Cairo\", \"Alexandria\", \"Giza\", \"Mansoura\", \"Cairo\"]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Tasks**\n",
        "\n",
        "1. Select the **\"City\"** column using both methods.\n",
        "2. Select the **first two rows** using `.iloc`.\n",
        "3. Select rows with labels **1 to 3** using `.loc`.\n",
        "4. Get the **Name and Age** columns for the first three rows.\n",
        "5. Select all rows where **Age > 25**.\n",
        "6. Select all rows where **City is Cairo OR Giza**.\n",
        "7. Select only the **Name** of people older than 28.\n",
        "8. Set \"Name\" as the index and access Omar’s row.\n",
        "9. Reset the index back to default integers.\n",
        "10. Slice the DataFrame to get rows **2 to 4** and only the \"City\" column.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Solutions\n",
        "\n",
        "```python\n",
        "# 1\n",
        "print(df[\"City\"])\n",
        "print(df.City)\n",
        "\n",
        "# 2\n",
        "print(df.iloc[0:2])\n",
        "\n",
        "# 3\n",
        "print(df.loc[1:3])\n",
        "\n",
        "# 4\n",
        "print(df.loc[0:2, [\"Name\", \"Age\"]])\n",
        "\n",
        "# 5\n",
        "print(df[df[\"Age\"] > 25])\n",
        "\n",
        "# 6\n",
        "print(df[df[\"City\"].isin([\"Cairo\", \"Giza\"])])\n",
        "\n",
        "# 7\n",
        "print(df.loc[df[\"Age\"] > 28, \"Name\"])\n",
        "\n",
        "# 8\n",
        "df2 = df.set_index(\"Name\")\n",
        "print(df2.loc[\"Omar\"])\n",
        "\n",
        "# 9\n",
        "print(df2.reset_index())\n",
        "\n",
        "# 10\n",
        "print(df.loc[2:4, [\"City\"]])\n",
        "```"
      ],
      "metadata": {
        "id": "2sq2aP0zOSIc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EjKfTuhfOXe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Data Cleaning in Pandas\n",
        "\n",
        "Real-world datasets are rarely perfect — they have **missing values, duplicates, wrong column names, and incorrect data types**. Pandas gives us tools to fix all that.\n",
        "\n",
        "---\n",
        "\n",
        "## 5.1 Handling Missing Values\n",
        "\n",
        "Missing values often appear as `NaN`.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = {\n",
        "    \"Name\": [\"Ali\", \"Sara\", \"Omar\", \"Nora\", \"Mona\"],\n",
        "    \"Age\": [25, np.nan, 22, 28, np.nan],\n",
        "    \"City\": [\"Cairo\", \"Alexandria\", None, \"Mansoura\", \"Cairo\"]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(df.isna())       # Check missing values (True/False)\n",
        "print(df.isna().sum()) # Count missing values per column\n",
        "\n",
        "# Fill missing values\n",
        "print(df.fillna(\"Unknown\"))      # Fill NaN with \"Unknown\"\n",
        "print(df[\"Age\"].fillna(df[\"Age\"].mean()))  # Replace NaN with mean Age\n",
        "\n",
        "# Drop missing values\n",
        "print(df.dropna())     # Drop rows with any NaN\n",
        "print(df.dropna(subset=[\"Age\"])) # Drop only if Age is missing\n",
        "```\n",
        "\n",
        "👉 **Tips & Tricks:**\n",
        "\n",
        "* Use `.fillna(method=\"ffill\")` (forward fill) or `.fillna(method=\"bfill\")` (backward fill) for sequential data.\n",
        "* `.dropna(axis=1)` removes **columns** with missing values.\n",
        "\n",
        "---\n",
        "\n",
        "## 5.2 Handling Duplicates\n",
        "\n",
        "```python\n",
        "data2 = {\n",
        "    \"Name\": [\"Ali\", \"Sara\", \"Omar\", \"Sara\"],\n",
        "    \"Age\": [25, 30, 22, 30],\n",
        "    \"City\": [\"Cairo\", \"Alexandria\", \"Giza\", \"Alexandria\"]\n",
        "}\n",
        "df2 = pd.DataFrame(data2)\n",
        "\n",
        "print(df2.duplicated())        # True if row is duplicate\n",
        "print(df2.drop_duplicates())   # Remove duplicates\n",
        "```\n",
        "\n",
        "👉 **Tips & Tricks:**\n",
        "\n",
        "* Keep first/last occurrence: `drop_duplicates(keep=\"first\")` or `keep=\"last\"`.\n",
        "* Drop based on certain columns:\n",
        "\n",
        "  ```python\n",
        "  df2.drop_duplicates(subset=[\"Name\"])\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "## 5.3 Renaming Columns\n",
        "\n",
        "```python\n",
        "df3 = df.rename(columns={\"Name\": \"FullName\", \"Age\": \"Years\"})\n",
        "print(df3)\n",
        "```\n",
        "\n",
        "👉 **Tips & Tricks:**\n",
        "\n",
        "* Rename in place:\n",
        "\n",
        "  ```python\n",
        "  df.rename(columns={\"Name\": \"FullName\"}, inplace=True)\n",
        "  ```\n",
        "* Use `df.columns = [list]` to rename all at once:\n",
        "\n",
        "  ```python\n",
        "  df.columns = [\"Name\", \"Age\", \"City\"]\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "## 5.4 Changing Data Types\n",
        "\n",
        "```python\n",
        "df[\"Age\"] = df[\"Age\"].astype(\"float\")   # Convert to float\n",
        "df[\"Age\"] = df[\"Age\"].astype(\"Int64\")   # Nullable integer\n",
        "```\n",
        "\n",
        "👉 **Tips & Tricks:**\n",
        "\n",
        "* Use `.astype(str)` to convert numbers to strings.\n",
        "* Use `pd.to_datetime(df[\"col\"])` for dates.\n",
        "* Use `pd.to_numeric(df[\"col\"], errors=\"coerce\")` to force invalid numbers into NaN.\n",
        "\n",
        "---\n",
        "\n",
        "# 📝 Exercises\n",
        "\n",
        "Using this DataFrame:\n",
        "\n",
        "```python\n",
        "data = {\n",
        "    \"Name\": [\"Ali\", \"Sara\", \"Omar\", \"Nora\", \"Sara\"],\n",
        "    \"Age\": [25, None, 22, 28, 30],\n",
        "    \"City\": [\"Cairo\", \"Alexandria\", None, \"Mansoura\", \"Alexandria\"]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Tasks**\n",
        "\n",
        "1. Check how many missing values are in each column.\n",
        "2. Fill missing Age values with the average Age.\n",
        "3. Fill missing City values with `\"Unknown\"`.\n",
        "4. Drop rows where `\"City\"` is missing.\n",
        "5. Identify which rows are duplicates.\n",
        "6. Remove duplicate rows, keeping the first occurrence.\n",
        "7. Remove duplicates based only on `\"Name\"`.\n",
        "8. Rename `\"Age\"` to `\"Years\"`.\n",
        "9. Change `\"Years\"` column to float type.\n",
        "10. Convert `\"City\"` to string type.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Solutions\n",
        "\n",
        "```python\n",
        "# 1\n",
        "print(df.isna().sum())\n",
        "\n",
        "# 2\n",
        "df[\"Age\"].fillna(df[\"Age\"].mean(), inplace=True)\n",
        "\n",
        "# 3\n",
        "df[\"City\"].fillna(\"Unknown\", inplace=True)\n",
        "\n",
        "# 4\n",
        "print(df.dropna(subset=[\"City\"]))\n",
        "\n",
        "# 5\n",
        "print(df.duplicated())\n",
        "\n",
        "# 6\n",
        "print(df.drop_duplicates())\n",
        "\n",
        "# 7\n",
        "print(df.drop_duplicates(subset=[\"Name\"]))\n",
        "\n",
        "# 8\n",
        "df.rename(columns={\"Age\": \"Years\"}, inplace=True)\n",
        "\n",
        "# 9\n",
        "df[\"Years\"] = df[\"Years\"].astype(float)\n",
        "\n",
        "# 10\n",
        "df[\"City\"] = df[\"City\"].astype(str)\n",
        "```"
      ],
      "metadata": {
        "id": "eT-CAUlDOX2z"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nyjq3Jy2OpGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perfect 😃 — let’s turn **Data Cleaning** into something closer to how it actually happens in real-world datasets. I’ll give you a couple of **mini case studies** that combine the techniques we just covered. That way you’ll see how missing values, duplicates, renaming, and type conversions play together.\n",
        "\n",
        "---\n",
        "\n",
        "# 🏥 Case Study 1: Hospital Patient Records\n",
        "\n",
        "Suppose you receive this dataset:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = {\n",
        "    \"PatientID\": [101, 102, 103, 104, 104, 105],\n",
        "    \"Name\": [\"Ali\", \"Sara\", \"Omar\", \"Nora\", \"Nora\", \"Mona\"],\n",
        "    \"Age\": [25, np.nan, 60, None, None, 45],\n",
        "    \"AdmissionDate\": [\"2023-01-10\", \"2023-02-15\", \"not_recorded\", \"2023-04-01\", \"2023-04-01\", \"2023-05-20\"],\n",
        "    \"City\": [\"Cairo\", \"Alex\", None, \"Cairo\", \"Cairo\", \"Mansoura\"]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "print(df)\n",
        "```\n",
        "\n",
        "**Issues we see immediately:**\n",
        "\n",
        "* Missing Ages\n",
        "* Invalid AdmissionDate (`\"not_recorded\"`)\n",
        "* Duplicate rows (PatientID 104 repeated)\n",
        "* City values inconsistent (`Alex` vs `Alexandria`)\n",
        "\n",
        "---\n",
        "\n",
        "### Cleaning Steps\n",
        "\n",
        "```python\n",
        "# 1. Handle missing values\n",
        "df[\"Age\"].fillna(df[\"Age\"].mean(), inplace=True)   # Fill age with mean\n",
        "\n",
        "# 2. Fix invalid dates\n",
        "df[\"AdmissionDate\"] = pd.to_datetime(df[\"AdmissionDate\"], errors=\"coerce\")\n",
        "\n",
        "# 3. Handle missing cities\n",
        "df[\"City\"].fillna(\"Unknown\", inplace=True)\n",
        "\n",
        "# 4. Standardize city names\n",
        "df[\"City\"].replace({\"Alex\": \"Alexandria\"}, inplace=True)\n",
        "\n",
        "# 5. Remove duplicates\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# 6. Set PatientID as index\n",
        "df.set_index(\"PatientID\", inplace=True)\n",
        "\n",
        "print(df)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "✅ **Result:** A clean dataset where\n",
        "\n",
        "* All patients have an Age\n",
        "* Dates are in proper `datetime` format\n",
        "* City names are consistent\n",
        "* No duplicate patients\n",
        "\n",
        "---\n",
        "\n",
        "# 🏢 Case Study 2: Employee Salary Records\n",
        "\n",
        "Dataset:\n",
        "\n",
        "```python\n",
        "data = {\n",
        "    \"EmpID\": [1, 2, 3, 4, 4, 5],\n",
        "    \"Full Name\": [\"Khaled\", \"Sara\", \"Omar\", \"Nora\", \"Nora\", \"Mona\"],\n",
        "    \"Salary\": [\"5000\", \"NaN\", \"7000\", \"8000\", \"8000\", \"ten thousand\"],\n",
        "    \"JoinDate\": [\"2021/01/10\", \"2021/02/15\", None, \"2021/04/01\", \"2021/04/01\", \"2021/05/20\"]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "print(df)\n",
        "```\n",
        "\n",
        "**Problems here:**\n",
        "\n",
        "* `\"Salary\"` column stored as strings\n",
        "* `\"NaN\"` and `\"ten thousand\"` are not valid numbers\n",
        "* Missing JoinDate\n",
        "* Duplicate employee ID (4)\n",
        "* Column name `\"Full Name\"` has a space\n",
        "\n",
        "---\n",
        "\n",
        "### Cleaning Steps\n",
        "\n",
        "```python\n",
        "# 1. Fix column names\n",
        "df.rename(columns={\"Full Name\": \"Name\"}, inplace=True)\n",
        "\n",
        "# 2. Convert Salary to numeric (invalid values → NaN)\n",
        "df[\"Salary\"] = pd.to_numeric(df[\"Salary\"], errors=\"coerce\")\n",
        "\n",
        "# 3. Fill missing salaries with median\n",
        "df[\"Salary\"].fillna(df[\"Salary\"].median(), inplace=True)\n",
        "\n",
        "# 4. Convert JoinDate to datetime\n",
        "df[\"JoinDate\"] = pd.to_datetime(df[\"JoinDate\"], errors=\"coerce\")\n",
        "\n",
        "# 5. Drop duplicates\n",
        "df = df.drop_duplicates(subset=[\"EmpID\"])\n",
        "\n",
        "# 6. Set EmpID as index\n",
        "df.set_index(\"EmpID\", inplace=True)\n",
        "\n",
        "print(df)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "✅ **Result:**\n",
        "\n",
        "* Clean `Salary` as numbers, with bad values handled\n",
        "* Proper `datetime` for JoinDate\n",
        "* No duplicate employees\n",
        "* Column names standardized\n",
        "\n",
        "---\n",
        "\n",
        "# 🌟 What you should notice\n",
        "\n",
        "* Real cleaning **always combines multiple steps**.\n",
        "* You often have to decide: **fill vs drop** missing values (depends on use case).\n",
        "* Text normalization (like `\"Alex\"` → `\"Alexandria\"`) is very common.\n",
        "* Changing types (`str → float`, `str → datetime`) is essential before analysis.\n",
        "\n",
        "---\n",
        "\n",
        "👉 Now here’s the fun part: do you want me to create a **bigger “challenge dataset”** (like a messy customer database with missing, wrong, and duplicate entries) and then **walk you through step-by-step cleaning it** as if you were preparing it for analysis? That would feel like a real project.\n"
      ],
      "metadata": {
        "id": "Hxe5FiwuOpcM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KYhBJ2UWO_2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🛒 Challenge Dataset: Messy Customer Records\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    \"CustomerID\": [1, 2, 2, 3, 4, 5, 6, 7, 8, 9],\n",
        "    \"Name\": [\"Ali\", \"Sara\", \"Sara\", \"Omar\", \"Nora\", \"Mona\", None, \"Khaled\", \"Sara\", \"Omar\"],\n",
        "    \"Age\": [25, \"Thirty\", None, 40, 29, 35, 22, None, 30, 40],\n",
        "    \"SignupDate\": [\"2021-01-10\", \"2021-02-30\", \"2021-02-15\", \"not_recorded\", None, \"2021-06-01\", \"2021/07/20\", \"2021-08-15\", \"2021-02-15\", \"2021-04-10\"],\n",
        "    \"Email\": [\"ali@example.com\", \"sara@\", \"sara@\", \"omar@example.com\", \"nora@example.com\", \"mona@example.com\", \"bad_email\", None, \"sara@\", \"omar@example.com\"],\n",
        "    \"City\": [\"Cairo\", \"Alex\", \"Alex\", None, \"Cairo\", \"Mansoura\", \"Mansoura\", \"Cairo\", \"Alexandria\", \"Cairo\"]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "print(df)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🚨 Problems spotted:\n",
        "\n",
        "* **Duplicates:** CustomerID 2 appears twice, Omar appears twice.\n",
        "* **Name column:** Missing values.\n",
        "* **Age column:** Mixed types (string `\"Thirty\"`, None, int).\n",
        "* **SignupDate column:** Invalid dates (`\"2021-02-30\"`, `\"not_recorded\"`, None).\n",
        "* **Email column:** Invalid emails (`\"sara@\"`, `\"bad_email\"`, None).\n",
        "* **City column:** Inconsistent naming (`\"Alex\"` vs `\"Alexandria\"`), missing values.\n",
        "\n",
        "---\n",
        "\n",
        "# 🛠 Step-by-Step Cleaning\n",
        "\n",
        "### 1. Handle duplicates\n",
        "\n",
        "```python\n",
        "df = df.drop_duplicates()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Fix missing names\n",
        "\n",
        "```python\n",
        "df[\"Name\"].fillna(\"Unknown\", inplace=True)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Fix ages\n",
        "\n",
        "```python\n",
        "# Convert to numeric, invalid → NaN\n",
        "df[\"Age\"] = pd.to_numeric(df[\"Age\"], errors=\"coerce\")\n",
        "\n",
        "# Fill missing ages with median\n",
        "df[\"Age\"].fillna(df[\"Age\"].median(), inplace=True)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Fix dates\n",
        "\n",
        "```python\n",
        "df[\"SignupDate\"] = pd.to_datetime(df[\"SignupDate\"], errors=\"coerce\")\n",
        "\n",
        "# Fill missing/invalid dates with the earliest valid signup date\n",
        "min_date = df[\"SignupDate\"].min()\n",
        "df[\"SignupDate\"].fillna(min_date, inplace=True)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 5. Clean emails\n",
        "\n",
        "```python\n",
        "# Simple rule: keep only if contains '@' and '.'\n",
        "df[\"Email\"] = df[\"Email\"].where(df[\"Email\"].str.contains(\"@\") & df[\"Email\"].str.contains(\"\\.\"), \"invalid@example.com\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 6. Normalize cities\n",
        "\n",
        "```python\n",
        "df[\"City\"].replace({\"Alex\": \"Alexandria\"}, inplace=True)\n",
        "df[\"City\"].fillna(\"Unknown\", inplace=True)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 7. Set a meaningful index\n",
        "\n",
        "```python\n",
        "df.set_index(\"CustomerID\", inplace=True)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# ✅ Final Cleaned DataFrame\n",
        "\n",
        "After cleaning, `df` is now consistent, with:\n",
        "\n",
        "* No duplicates\n",
        "* Valid numbers for Age\n",
        "* Valid dates in SignupDate\n",
        "* Standardized City names\n",
        "* Emails either valid or flagged as `\"invalid@example.com\"`\n",
        "\n",
        "---\n",
        "\n",
        "👉 That’s a **realistic data cleaning workflow**. You’ve touched:\n",
        "\n",
        "* Missing values\n",
        "* Duplicates\n",
        "* Type conversion\n",
        "* String cleanup\n",
        "* Normalization\n"
      ],
      "metadata": {
        "id": "nzM1nDfDPAOU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oehjS3GWPHGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Operations in Pandas\n",
        "\n",
        "---\n",
        "\n",
        "## 6.1 Arithmetic Operations\n",
        "\n",
        "Pandas lets you do math directly on columns (like Excel formulas).\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    \"Name\": [\"Ali\", \"Sara\", \"Omar\", \"Nora\"],\n",
        "    \"Math\": [80, 90, 75, 60],\n",
        "    \"Science\": [85, 95, 70, 65]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Column-wise arithmetic\n",
        "df[\"Total\"] = df[\"Math\"] + df[\"Science\"]\n",
        "df[\"Average\"] = df[\"Total\"] / 2\n",
        "\n",
        "print(df)\n",
        "\n",
        "# Broadcasting with scalars\n",
        "df[\"Bonus\"] = df[\"Math\"] * 1.1\n",
        "```\n",
        "\n",
        "👉 **Tips & Tricks:**\n",
        "\n",
        "* Operations are vectorized → much faster than loops.\n",
        "* Use `.add(), .sub(), .mul(), .div()` for element-wise with `fill_value` in case of NaN.\n",
        "\n",
        "---\n",
        "\n",
        "## 6.2 String Operations (`.str`)\n",
        "\n",
        "When dealing with text columns, use `.str` for transformations.\n",
        "\n",
        "```python\n",
        "df2 = pd.DataFrame({\"City\": [\"cairo\", \"Alexandria\", \"giza\", \"Mansoura\"]})\n",
        "\n",
        "print(df2[\"City\"].str.upper())     # Convert to uppercase\n",
        "print(df2[\"City\"].str.lower())     # Convert to lowercase\n",
        "print(df2[\"City\"].str.len())       # Length of each string\n",
        "print(df2[\"City\"].str.contains(\"a\"))  # Boolean mask\n",
        "```\n",
        "\n",
        "👉 **Tips & Tricks:**\n",
        "\n",
        "* Use `.str.replace(\"old\", \"new\")` for corrections.\n",
        "* `.str.strip()` removes spaces; `.str.split(\",\")` splits strings.\n",
        "\n",
        "---\n",
        "\n",
        "## 6.3 Apply Functions (`apply`, `map`, `applymap`)\n",
        "\n",
        "You can apply custom logic to rows, columns, or individual elements.\n",
        "\n",
        "```python\n",
        "# Column-wise with apply\n",
        "df[\"Grade\"] = df[\"Average\"].apply(lambda x: \"Pass\" if x >= 75 else \"Fail\")\n",
        "\n",
        "# Element-wise with map (for Series)\n",
        "df[\"NameLength\"] = df[\"Name\"].map(len)\n",
        "\n",
        "# Element-wise with applymap (for entire DataFrame)\n",
        "df_numeric = df[[\"Math\", \"Science\"]]\n",
        "print(df_numeric.applymap(lambda x: x + 5))  # Add 5 to all numbers\n",
        "```\n",
        "\n",
        "👉 **Tips & Tricks:**\n",
        "\n",
        "* `applymap` → works on entire DataFrame, element by element.\n",
        "* `apply` → works on Series (or DataFrame row/col).\n",
        "* `map` → only for Series.\n",
        "\n",
        "---\n",
        "\n",
        "## 6.4 Sorting (`sort_values`, `sort_index`)\n",
        "\n",
        "Sorting is essential for organizing data.\n",
        "\n",
        "```python\n",
        "# Sort by column values\n",
        "print(df.sort_values(\"Average\", ascending=False))\n",
        "\n",
        "# Sort by multiple columns\n",
        "print(df.sort_values([\"Average\", \"Math\"], ascending=[False, True]))\n",
        "\n",
        "# Sort by index\n",
        "print(df.sort_index())\n",
        "```\n",
        "\n",
        "👉 **Tips & Tricks:**\n",
        "\n",
        "* Use `ascending=False` for descending order.\n",
        "* `na_position=\"first\"` moves NaN to the top.\n",
        "\n",
        "---\n",
        "\n",
        "# 📝 Exercises\n",
        "\n",
        "Using this dataset:\n",
        "\n",
        "```python\n",
        "data = {\n",
        "    \"Name\": [\"Ali\", \"Sara\", \"Omar\", \"Nora\", \"Mona\"],\n",
        "    \"Math\": [80, 90, 75, 60, 85],\n",
        "    \"Science\": [85, 95, 70, 65, 88],\n",
        "    \"City\": [\"cairo\", \"Alexandria\", \"giza\", \"Mansoura\", \"Cairo\"]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Tasks**\n",
        "\n",
        "1. Create a new column `\"Total\"` = Math + Science.\n",
        "2. Create a new column `\"Average\"` = Total / 2.\n",
        "3. Multiply all `\"Math\"` scores by 1.1 (bonus).\n",
        "4. Convert all `\"City\"` names to uppercase.\n",
        "5. Count the number of characters in each `\"City\"`.\n",
        "6. Create a new column `\"Result\"` → `\"Pass\"` if Average ≥ 80, else `\"Fail\"`.\n",
        "7. Add a column `\"NameLength\"` containing the length of each name.\n",
        "8. Add 10 to all numbers in both `\"Math\"` and `\"Science\"` columns.\n",
        "9. Sort the DataFrame by `\"Average\"` in descending order.\n",
        "10. Sort the DataFrame by `\"City\"` alphabetically.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Solutions\n",
        "\n",
        "```python\n",
        "# 1\n",
        "df[\"Total\"] = df[\"Math\"] + df[\"Science\"]\n",
        "\n",
        "# 2\n",
        "df[\"Average\"] = df[\"Total\"] / 2\n",
        "\n",
        "# 3\n",
        "df[\"MathBonus\"] = df[\"Math\"] * 1.1\n",
        "\n",
        "# 4\n",
        "df[\"CityUpper\"] = df[\"City\"].str.upper()\n",
        "\n",
        "# 5\n",
        "df[\"CityLength\"] = df[\"City\"].str.len()\n",
        "\n",
        "# 6\n",
        "df[\"Result\"] = df[\"Average\"].apply(lambda x: \"Pass\" if x >= 80 else \"Fail\")\n",
        "\n",
        "# 7\n",
        "df[\"NameLength\"] = df[\"Name\"].map(len)\n",
        "\n",
        "# 8\n",
        "df[[\"Math\", \"Science\"]] = df[[\"Math\", \"Science\"]].applymap(lambda x: x + 10)\n",
        "\n",
        "# 9\n",
        "print(df.sort_values(\"Average\", ascending=False))\n",
        "\n",
        "# 10\n",
        "print(df.sort_values(\"City\"))\n",
        "```\n"
      ],
      "metadata": {
        "id": "S9tKoEXRPH-U"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XC0jJeXJPdrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 💼 Case Study: Employee Performance Dataset\n",
        "\n",
        "You receive this raw dataset from HR:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    \"EmpID\": [101, 102, 103, 104, 105],\n",
        "    \"Name\": [\"ali \", \"SARA\", \"Omar\", \"nora\", \"Mona\"],\n",
        "    \"Department\": [\"sales\", \"sales\", \"tech\", \"tech\", \"sales\"],\n",
        "    \"BaseSalary\": [5000, 6000, 7000, 6500, 6200],\n",
        "    \"Bonus%\": [10, 15, 5, 12, 20],\n",
        "    \"Projects\": [3, 5, 2, 4, 6]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "print(df)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🚨 Problems to solve:\n",
        "\n",
        "* `Name` column is inconsistent (extra spaces, mixed case).\n",
        "* We need **total salary after bonus**.\n",
        "* We want to **categorize employees** as `\"High Performer\"` or `\"Regular\"` based on projects.\n",
        "* Need to **sort employees** by performance and salary.\n",
        "\n",
        "---\n",
        "\n",
        "# 🛠 Cleaning & Operations\n",
        "\n",
        "### 1. Fix Names (string ops)\n",
        "\n",
        "```python\n",
        "df[\"Name\"] = df[\"Name\"].str.strip().str.title()\n",
        "```\n",
        "\n",
        "✅ Removes spaces and makes names consistent (`\"ali \"` → `\"Ali\"`, `\"SARA\"` → `\"Sara\"`).\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Calculate Total Salary (arithmetic ops)\n",
        "\n",
        "```python\n",
        "df[\"TotalSalary\"] = df[\"BaseSalary\"] + (df[\"BaseSalary\"] * df[\"Bonus%\"] / 100)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Categorize Employees (apply function)\n",
        "\n",
        "```python\n",
        "df[\"Performance\"] = df[\"Projects\"].apply(lambda x: \"High Performer\" if x >= 4 else \"Regular\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Sort by Performance then Salary\n",
        "\n",
        "```python\n",
        "df_sorted = df.sort_values([\"Performance\", \"TotalSalary\"], ascending=[False, False])\n",
        "print(df_sorted)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# ✅ Final Result (Clean + Analyzed)\n",
        "\n",
        "| EmpID | Name | Department | BaseSalary | Bonus% | Projects | TotalSalary | Performance    |\n",
        "| ----- | ---- | ---------- | ---------- | ------ | -------- | ----------- | -------------- |\n",
        "| 105   | Mona | sales      | 6200       | 20     | 6        | 7440        | High Performer |\n",
        "| 102   | Sara | sales      | 6000       | 15     | 5        | 6900        | High Performer |\n",
        "| 104   | Nora | tech       | 6500       | 12     | 4        | 7280        | High Performer |\n",
        "| 103   | Omar | tech       | 7000       | 5      | 2        | 7350        | Regular        |\n",
        "| 101   | Ali  | sales      | 5000       | 10     | 3        | 5500        | Regular        |\n",
        "\n",
        "---\n",
        "\n",
        "# 🌟 What you did here\n",
        "\n",
        "* **String ops**: Cleaned inconsistent names.\n",
        "* **Arithmetic ops**: Calculated salaries.\n",
        "* **Apply functions**: Created performance categories.\n",
        "* **Sorting**: Ranked employees by performance + pay.\n",
        "\n",
        "This is exactly how operations come together in practice.\n"
      ],
      "metadata": {
        "id": "r1yKY-3VPeHb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sunhjDo0P09E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Operations in Pandas\n",
        "\n",
        "Pandas allows you to perform a wide range of operations on DataFrames and Series. These include **arithmetic operations, string operations, applying functions, and sorting.**\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Arithmetic Operations\n",
        "\n",
        "Pandas supports **element-wise arithmetic operations** between DataFrames and Series.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'A': [10, 20, 30],\n",
        "    'B': [1, 2, 3]\n",
        "})\n",
        "\n",
        "print(df['A'] + df['B'])   # Addition\n",
        "print(df['A'] - df['B'])   # Subtraction\n",
        "print(df['A'] * df['B'])   # Multiplication\n",
        "print(df['A'] / df['B'])   # Division\n",
        "```\n",
        "\n",
        "👉 **Tip:** Pandas automatically **aligns by index**. If the indices don’t match, it fills missing values with NaN.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. String Operations (`.str`)\n",
        "\n",
        "String operations are applied to **Series of strings** using `.str`.\n",
        "\n",
        "```python\n",
        "s = pd.Series([\"Pandas\", \"is\", \"awesome\", \"python\"])\n",
        "print(s.str.upper())     # Convert to uppercase\n",
        "print(s.str.contains(\"a\"))  # Check if string contains 'a'\n",
        "print(s.str.len())       # Length of each string\n",
        "print(s.str.replace(\"python\", \"great\"))\n",
        "```\n",
        "\n",
        "👉 **Tip:** You can chain multiple string operations together:\n",
        "\n",
        "```python\n",
        "s.str.lower().str.replace(\"python\", \"pandas\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Apply Functions (`apply`, `map`, `applymap`)\n",
        "\n",
        "* **apply()** → Works on **rows or columns** of a DataFrame.\n",
        "* **map()** → Works on a **Series** (1D).\n",
        "* **applymap()** → Works on **element-wise DataFrame**.\n",
        "\n",
        "```python\n",
        "# Using apply on DataFrame\n",
        "print(df.apply(sum))   # Sum of each column\n",
        "\n",
        "# Using map on Series\n",
        "print(df['A'].map(lambda x: x * 2))\n",
        "\n",
        "# Using applymap on DataFrame\n",
        "print(df.applymap(lambda x: x ** 2))\n",
        "```\n",
        "\n",
        "👉 **Tip:** Use **vectorized operations** whenever possible—they are faster than apply/map.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Sorting\n",
        "\n",
        "### Sort by column values\n",
        "\n",
        "```python\n",
        "print(df.sort_values(by='A', ascending=False))\n",
        "```\n",
        "\n",
        "### Sort by index\n",
        "\n",
        "```python\n",
        "print(df.sort_index())\n",
        "```\n",
        "\n",
        "👉 **Tip:** Use `df.sort_values(by=['A', 'B'])` for **multi-column sorting**.\n",
        "\n",
        "---\n",
        "\n",
        "# ✅ Exercises (with Solutions)\n",
        "\n",
        "### **Exercise 1**\n",
        "\n",
        "Create a DataFrame with columns `Math`, `Science`, `English`. Add 10 to all values in `Math`.\n",
        "\n",
        "✅ **Solution:**\n",
        "\n",
        "```python\n",
        "df = pd.DataFrame({'Math': [50, 60, 70], 'Science': [80, 90, 100], 'English': [65, 75, 85]})\n",
        "df['Math'] = df['Math'] + 10\n",
        "print(df)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Exercise 2**\n",
        "\n",
        "Check which strings contain the word `\"data\"` in a Series.\n",
        "\n",
        "✅ **Solution:**\n",
        "\n",
        "```python\n",
        "s = pd.Series([\"data science\", \"machine learning\", \"deep learning\"])\n",
        "print(s.str.contains(\"data\"))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Exercise 3**\n",
        "\n",
        "Convert all strings in a Series to title case.\n",
        "\n",
        "✅ **Solution:**\n",
        "\n",
        "```python\n",
        "s = pd.Series([\"python is fun\", \"pandas tutorial\"])\n",
        "print(s.str.title())\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Exercise 4**\n",
        "\n",
        "Square all numbers in a DataFrame using `applymap`.\n",
        "\n",
        "✅ **Solution:**\n",
        "\n",
        "```python\n",
        "df = pd.DataFrame({'X': [1, 2, 3], 'Y': [4, 5, 6]})\n",
        "print(df.applymap(lambda x: x ** 2))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Exercise 5**\n",
        "\n",
        "Get the length of each string in a Series.\n",
        "\n",
        "✅ **Solution:**\n",
        "\n",
        "```python\n",
        "s = pd.Series([\"apple\", \"banana\", \"cherry\"])\n",
        "print(s.str.len())\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Exercise 6**\n",
        "\n",
        "Sort a DataFrame by column `Age` in descending order.\n",
        "\n",
        "✅ **Solution:**\n",
        "\n",
        "```python\n",
        "df = pd.DataFrame({'Name': ['Ali', 'Sara', 'John'], 'Age': [25, 30, 22]})\n",
        "print(df.sort_values(by='Age', ascending=False))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Exercise 7**\n",
        "\n",
        "Apply a function that adds `100` to all values in column `Salary`.\n",
        "\n",
        "✅ **Solution:**\n",
        "\n",
        "```python\n",
        "df = pd.DataFrame({'Salary': [1000, 2000, 3000]})\n",
        "df['Salary'] = df['Salary'].apply(lambda x: x + 100)\n",
        "print(df)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Exercise 8**\n",
        "\n",
        "Convert all values in a Series to lowercase.\n",
        "\n",
        "✅ **Solution:**\n",
        "\n",
        "```python\n",
        "s = pd.Series([\"HELLO\", \"WORLD\", \"PYTHON\"])\n",
        "print(s.str.lower())\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Exercise 9**\n",
        "\n",
        "Check if each string in a Series starts with `\"p\"`.\n",
        "\n",
        "✅ **Solution:**\n",
        "\n",
        "```python\n",
        "s = pd.Series([\"python\", \"Pandas\", \"Numpy\"])\n",
        "print(s.str.startswith(\"p\"))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Exercise 10**\n",
        "\n",
        "Sort a DataFrame by multiple columns (`Score` ascending, `Age` descending).\n",
        "\n",
        "✅ **Solution:**\n",
        "\n",
        "```python\n",
        "df = pd.DataFrame({\n",
        "    'Name': ['A', 'B', 'C'],\n",
        "    'Score': [90, 80, 90],\n",
        "    'Age': [20, 25, 22]\n",
        "})\n",
        "print(df.sort_values(by=['Score', 'Age'], ascending=[True, False]))\n",
        "```"
      ],
      "metadata": {
        "id": "PNQ3vCFGP1j8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = {\n",
        "    \"Department\": [\"IT\", \"IT\", \"HR\", \"HR\", \"Finance\", \"Finance\"],\n",
        "    \"Employee\": [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"],\n",
        "    \"Salary\": [60000, 65000, 55000, 52000, 70000, 72000],\n",
        "    \"Bonus\": [5000, 6000, 4000, 4500, 7000, 7500]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_-5kSN1QHei",
        "outputId": "4c4fadd3-c33f-405f-f4b0-6cda410b533c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Department Employee  Salary  Bonus\n",
            "0         IT        A   60000   5000\n",
            "1         IT        B   65000   6000\n",
            "2         HR        C   55000   4000\n",
            "3         HR        D   52000   4500\n",
            "4    Finance        E   70000   7000\n",
            "5    Finance        F   72000   7500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grouped = df.groupby(\"Department\")\n",
        "print(grouped[\"Salary\"].mean())  # average salary per department"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mg31yZfJQIAN",
        "outputId": "1b1b579b-053f-4428-dab3-ce9b027e5e1e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Department\n",
            "Finance    71000.0\n",
            "HR         53500.0\n",
            "IT         62500.0\n",
            "Name: Salary, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pivot = df.pivot_table(\n",
        "    values=\"Salary\",\n",
        "    index=\"Department\",\n",
        "    aggfunc=\"mean\"\n",
        ")\n",
        "print(pivot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFN999K_QnCD",
        "outputId": "8b057db9-4412-4888-f479-b7e0b1bd8345"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             Salary\n",
            "Department         \n",
            "Finance     71000.0\n",
            "HR          53500.0\n",
            "IT          62500.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wVAnQribQ0wU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 7. Grouping & Aggregation in Pandas\n",
        "\n",
        "Grouping and aggregation are powerful ways to **summarize** and **analyze** data. They let you split data into groups, apply computations, and combine results efficiently.\n",
        "\n",
        "---\n",
        "\n",
        "## 7.1 GroupBy Basics\n",
        "\n",
        "The main idea:\n",
        "**Split → Apply → Combine**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    \"Department\": [\"IT\", \"IT\", \"HR\", \"HR\", \"Finance\", \"Finance\"],\n",
        "    \"Employee\": [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"],\n",
        "    \"Salary\": [60000, 65000, 55000, 52000, 70000, 72000],\n",
        "    \"Bonus\": [5000, 6000, 4000, 4500, 7000, 7500]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(df)\n",
        "```\n",
        "\n",
        "👉 **Group by Department**:\n",
        "\n",
        "```python\n",
        "grouped = df.groupby(\"Department\")\n",
        "print(grouped[\"Salary\"].mean())  # average salary per department\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 7.2 Aggregate Functions\n",
        "\n",
        "Aggregation is applying a **summary function** on groups.\n",
        "Common aggregate functions:\n",
        "\n",
        "* `sum()` – total\n",
        "* `mean()` – average\n",
        "* `count()` – number of items\n",
        "* `min(), max()` – smallest/largest\n",
        "* `std(), var()` – standard deviation, variance\n",
        "* `median()` – middle value\n",
        "\n",
        "```python\n",
        "# Total salary per department\n",
        "print(df.groupby(\"Department\")[\"Salary\"].sum())\n",
        "\n",
        "# Multiple aggregations\n",
        "print(df.groupby(\"Department\")[\"Salary\"].agg([\"mean\", \"sum\", \"max\"]))\n",
        "```\n",
        "\n",
        "👉 Using **different functions on different columns**:\n",
        "\n",
        "```python\n",
        "print(df.groupby(\"Department\").agg({\n",
        "    \"Salary\": \"mean\",\n",
        "    \"Bonus\": \"sum\"\n",
        "}))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 7.3 Pivot Tables\n",
        "\n",
        "Pivot tables are like Excel pivot tables—summarize data by categories.\n",
        "\n",
        "```python\n",
        "pivot = df.pivot_table(\n",
        "    values=\"Salary\",\n",
        "    index=\"Department\",\n",
        "    aggfunc=\"mean\"\n",
        ")\n",
        "print(pivot)\n",
        "```\n",
        "\n",
        "👉 With multiple values and functions:\n",
        "\n",
        "```python\n",
        "pivot2 = df.pivot_table(\n",
        "    values=[\"Salary\", \"Bonus\"],\n",
        "    index=\"Department\",\n",
        "    aggfunc={\"Salary\": \"mean\", \"Bonus\": \"sum\"}\n",
        ")\n",
        "print(pivot2)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 💡 Tips & Tricks\n",
        "\n",
        "1. **`as_index=False`** keeps grouped results as a DataFrame instead of setting group keys as index.\n",
        "\n",
        "   ```python\n",
        "   df.groupby(\"Department\", as_index=False).mean()\n",
        "   ```\n",
        "\n",
        "2. Use **`reset_index()`** after grouping to flatten the result.\n",
        "\n",
        "3. **Pivot tables** are more flexible than groupby for multi-level summaries.\n",
        "\n",
        "4. Use **`margins=True`** in `pivot_table` to get totals.\n",
        "\n",
        "   ```python\n",
        "   df.pivot_table(values=\"Salary\", index=\"Department\", aggfunc=\"mean\", margins=True)\n",
        "   ```\n",
        "\n",
        "5. For **faster computation on large datasets**, use `.agg()` with dictionaries instead of chaining multiple groupby operations.\n",
        "\n",
        "---\n",
        "\n",
        "## 📝 Exercises\n",
        "\n",
        "### Exercise 1\n",
        "\n",
        "Group the employees by `Department` and calculate the **average bonus**.\n",
        "\n",
        "### Exercise 2\n",
        "\n",
        "Find the **total salary** paid by each department.\n",
        "\n",
        "### Exercise 3\n",
        "\n",
        "Count how many employees work in each department.\n",
        "\n",
        "### Exercise 4\n",
        "\n",
        "Find the **maximum bonus** in each department.\n",
        "\n",
        "### Exercise 5\n",
        "\n",
        "Group by `Department` and calculate both **mean Salary** and **sum Bonus**.\n",
        "\n",
        "### Exercise 6\n",
        "\n",
        "Create a pivot table showing the **average Salary** per Department.\n",
        "\n",
        "### Exercise 7\n",
        "\n",
        "Modify the pivot table to show both **average Salary** and **sum of Bonus** per Department.\n",
        "\n",
        "### Exercise 8\n",
        "\n",
        "Add `margins=True` to the pivot table and observe the total row.\n",
        "\n",
        "### Exercise 9\n",
        "\n",
        "Use `.agg()` to calculate **min Salary** and **max Bonus** per Department.\n",
        "\n",
        "### Exercise 10\n",
        "\n",
        "Group by Department and return results as a normal DataFrame (not index-based).\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Solutions\n",
        "\n",
        "```python\n",
        "# 1. Average bonus\n",
        "print(df.groupby(\"Department\")[\"Bonus\"].mean())\n",
        "\n",
        "# 2. Total salary\n",
        "print(df.groupby(\"Department\")[\"Salary\"].sum())\n",
        "\n",
        "# 3. Employee count\n",
        "print(df.groupby(\"Department\")[\"Employee\"].count())\n",
        "\n",
        "# 4. Max bonus\n",
        "print(df.groupby(\"Department\")[\"Bonus\"].max())\n",
        "\n",
        "# 5. Mean Salary + Sum Bonus\n",
        "print(df.groupby(\"Department\").agg({\"Salary\": \"mean\", \"Bonus\": \"sum\"}))\n",
        "\n",
        "# 6. Pivot avg salary\n",
        "print(df.pivot_table(values=\"Salary\", index=\"Department\", aggfunc=\"mean\"))\n",
        "\n",
        "# 7. Pivot salary+bonus\n",
        "print(df.pivot_table(values=[\"Salary\", \"Bonus\"], index=\"Department\",\n",
        "                     aggfunc={\"Salary\": \"mean\", \"Bonus\": \"sum\"}))\n",
        "\n",
        "# 8. Pivot with margins\n",
        "print(df.pivot_table(values=\"Salary\", index=\"Department\",\n",
        "                     aggfunc=\"mean\", margins=True))\n",
        "\n",
        "# 9. Min salary & Max bonus\n",
        "print(df.groupby(\"Department\").agg({\"Salary\": \"min\", \"Bonus\": \"max\"}))\n",
        "\n",
        "# 10. Keep DataFrame format\n",
        "print(df.groupby(\"Department\", as_index=False).mean())\n",
        "```\n"
      ],
      "metadata": {
        "id": "fcMIcP2CQ1Oz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mfrHW1uPQ9cy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Merging, Joining & Concatenation\n",
        "\n",
        "## 1. Concatenation (`pd.concat`)\n",
        "\n",
        "* Used to **stack** DataFrames vertically (row-wise) or horizontally (column-wise).\n",
        "* Think of it as “gluing” datasets together.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2'], 'B': ['B0', 'B1', 'B2']})\n",
        "df2 = pd.DataFrame({'A': ['A3', 'A4', 'A5'], 'B': ['B3', 'B4', 'B5']})\n",
        "\n",
        "# Vertical concatenation (default: axis=0 → rows)\n",
        "result = pd.concat([df1, df2])\n",
        "\n",
        "# Horizontal concatenation (axis=1 → columns)\n",
        "result_h = pd.concat([df1, df2], axis=1)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Merge (`pd.merge`)\n",
        "\n",
        "* Similar to **SQL joins**.\n",
        "* Combines DataFrames on **common columns** or **indices**.\n",
        "\n",
        "```python\n",
        "left = pd.DataFrame({'id': [1, 2, 3], 'name': ['Alice', 'Bob', 'Charlie']})\n",
        "right = pd.DataFrame({'id': [1, 2, 4], 'score': [85, 90, 75]})\n",
        "\n",
        "# Inner Join (default)\n",
        "pd.merge(left, right, on='id', how='inner')\n",
        "\n",
        "# Left Join\n",
        "pd.merge(left, right, on='id', how='left')\n",
        "\n",
        "# Right Join\n",
        "pd.merge(left, right, on='id', how='right')\n",
        "\n",
        "# Outer Join\n",
        "pd.merge(left, right, on='id', how='outer')\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Join (`DataFrame.join`)\n",
        "\n",
        "* A shortcut for joining on **index**.\n",
        "* Works best when combining a main DataFrame with a **lookup table**.\n",
        "\n",
        "```python\n",
        "left = pd.DataFrame({'name': ['Alice', 'Bob']}, index=[1, 2])\n",
        "right = pd.DataFrame({'score': [85, 90]}, index=[1, 2])\n",
        "\n",
        "# Joins on index\n",
        "left.join(right)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Differences Between concat, merge, join\n",
        "\n",
        "| Function   | Use Case                                |\n",
        "| ---------- | --------------------------------------- |\n",
        "| **concat** | Just stack/append along rows or columns |\n",
        "| **merge**  | SQL-like joins using keys (columns)     |\n",
        "| **join**   | Convenient method to join on **index**  |\n",
        "\n",
        "---\n",
        "\n",
        "## 🔑 Tips & Tricks\n",
        "\n",
        "* Use `ignore_index=True` with `concat` to reindex automatically.\n",
        "* Use `indicator=True` in `merge` to see join origin (`left_only`, `right_only`, `both`).\n",
        "* Rename overlapping columns with `suffixes=('_left', '_right')`.\n",
        "* Always check for duplicate keys before merging to avoid data explosion.\n",
        "\n",
        "---\n",
        "\n",
        "## 📝 Exercises\n",
        "\n",
        "### Exercise 1\n",
        "\n",
        "Concatenate the following two DataFrames row-wise and column-wise:\n",
        "\n",
        "```python\n",
        "df1 = pd.DataFrame({'X': [1, 2], 'Y': [3, 4]})\n",
        "df2 = pd.DataFrame({'X': [5, 6], 'Y': [7, 8]})\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 2\n",
        "\n",
        "Perform an **inner join** between:\n",
        "\n",
        "```python\n",
        "df1 = pd.DataFrame({'id': [1, 2, 3], 'value': ['A', 'B', 'C']})\n",
        "df2 = pd.DataFrame({'id': [2, 3, 4], 'score': [10, 20, 30]})\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 3\n",
        "\n",
        "Do a **left join** on the same DataFrames.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 4\n",
        "\n",
        "Perform an **outer join** on the same DataFrames.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 5\n",
        "\n",
        "Join two DataFrames on their **index**:\n",
        "\n",
        "```python\n",
        "df1 = pd.DataFrame({'A': ['foo', 'bar']}, index=[1, 2])\n",
        "df2 = pd.DataFrame({'B': ['baz', 'qux']}, index=[1, 2])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 6\n",
        "\n",
        "Concatenate three small DataFrames (`df1`, `df2`, `df3`) along **rows**.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 7\n",
        "\n",
        "Merge with different column names:\n",
        "\n",
        "```python\n",
        "df1 = pd.DataFrame({'emp_id': [1, 2], 'name': ['Alice', 'Bob']})\n",
        "df2 = pd.DataFrame({'id': [1, 2], 'salary': [1000, 1500]})\n",
        "```\n",
        "\n",
        "(Hint: use `left_on` and `right_on`.)\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 8\n",
        "\n",
        "Perform a merge with `indicator=True` and explain what the output column means.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 9\n",
        "\n",
        "Use `concat` with `keys=['df1', 'df2']` to create a hierarchical index.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 10\n",
        "\n",
        "Create two DataFrames with duplicate keys and show how merging causes **data duplication**."
      ],
      "metadata": {
        "id": "nforPs7TQ90B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Merging, Joining & Concatenation in Pandas\n",
        "\n",
        "Working with multiple datasets is one of the most common tasks in data analysis. Pandas provides **powerful tools** to combine DataFrames in different ways, similar to SQL.\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 1. Concatenation (`pd.concat`)\n",
        "\n",
        "* **Definition**: Combines multiple DataFrames **vertically** (stacking rows) or **horizontally** (stacking columns).\n",
        "* **Syntax**:\n",
        "\n",
        "  ```python\n",
        "  pd.concat([df1, df2], axis=0)   # Vertical (default)\n",
        "  pd.concat([df1, df2], axis=1)   # Horizontal\n",
        "  ```\n",
        "* **Options**:\n",
        "\n",
        "  * `ignore_index=True` → resets index.\n",
        "  * `keys` → adds hierarchical index.\n",
        "\n",
        "✅ **Tip**: Use `axis=0` for more rows, `axis=1` for more columns.\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 2. Merge (`pd.merge`)\n",
        "\n",
        "* **Definition**: Combines DataFrames based on one or more **common columns or indices**.\n",
        "* **Syntax**:\n",
        "\n",
        "  ```python\n",
        "  pd.merge(df1, df2, on=\"key\")\n",
        "  ```\n",
        "* **Join types (like SQL)**:\n",
        "\n",
        "  * `how=\"inner\"` (default): Keep only matching rows.\n",
        "  * `how=\"left\"`: Keep all rows from left, add matching from right.\n",
        "  * `how=\"right\"`: Keep all rows from right, add matching from left.\n",
        "  * `how=\"outer\"`: Keep all rows from both.\n",
        "\n",
        "✅ **Tip**: Always check for **duplicate column names** — Pandas will add suffixes like `_x`, `_y`.\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 3. Join (`df.join`)\n",
        "\n",
        "* **Definition**: A shortcut for merging, often using **index** instead of columns.\n",
        "* **Syntax**:\n",
        "\n",
        "  ```python\n",
        "  df1.join(df2, how=\"left\")\n",
        "  ```\n",
        "* Works well when you want to combine DataFrames **by index**.\n",
        "\n",
        "✅ **Tip**: Use `.set_index()` first if you want to join on a specific column.\n",
        "\n",
        "---\n",
        "\n",
        "## 📊 Example: SQL-style joins\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "employees = pd.DataFrame({\n",
        "    \"id\": [1, 2, 3, 4],\n",
        "    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"]\n",
        "})\n",
        "\n",
        "salaries = pd.DataFrame({\n",
        "    \"id\": [1, 2, 4, 5],\n",
        "    \"salary\": [50000, 60000, 70000, 80000]\n",
        "})\n",
        "\n",
        "# INNER JOIN\n",
        "print(pd.merge(employees, salaries, on=\"id\", how=\"inner\"))\n",
        "\n",
        "# LEFT JOIN\n",
        "print(pd.merge(employees, salaries, on=\"id\", how=\"left\"))\n",
        "\n",
        "# OUTER JOIN\n",
        "print(pd.merge(employees, salaries, on=\"id\", how=\"outer\"))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# 💡 Tips & Tricks\n",
        "\n",
        "* Use `indicator=True` in `merge` → shows which rows came from which DataFrame.\n",
        "* Use `concat` with `keys` to distinguish data sources.\n",
        "* Use `suffixes=('_left', '_right')` to avoid confusion in column names after merge.\n",
        "* For performance, **set indexes** before merging on large DataFrames.\n",
        "\n",
        "---\n",
        "\n",
        "# 📝 Exercises\n",
        "\n",
        "### **Exercise 1: Concatenation of rows**\n",
        "\n",
        "Combine two DataFrames vertically.\n",
        "\n",
        "```python\n",
        "df1 = pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]})\n",
        "df2 = pd.DataFrame({\"A\": [5, 6], \"B\": [7, 8]})\n",
        "```\n",
        "\n",
        "✅ **Solution:**\n",
        "\n",
        "```python\n",
        "pd.concat([df1, df2], ignore_index=True)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Exercise 2: Concatenation of columns**\n",
        "\n",
        "Stack `df1` and `df2` side by side.\n",
        "\n",
        "✅ **Solution:**\n",
        "\n",
        "```python\n",
        "pd.concat([df1, df2], axis=1)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Exercise 3: Inner merge**\n",
        "\n",
        "Merge employees and salaries only where IDs match.\n",
        "\n",
        "✅ **Solution:**\n",
        "\n",
        "```python\n",
        "pd.merge(employees, salaries, on=\"id\", how=\"inner\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Exercise 4: Left merge**\n",
        "\n",
        "Keep all employees, even if they don’t have salaries.\n",
        "\n",
        "✅ **Solution:**\n",
        "\n",
        "```python\n",
        "pd.merge(employees, salaries, on=\"id\", how=\"left\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Exercise 5: Outer merge**\n",
        "\n",
        "Show all employees and all salaries (IDs may not match).\n",
        "\n",
        "✅ **Solution:**\n",
        "\n",
        "```python\n",
        "pd.merge(employees, salaries, on=\"id\", how=\"outer\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Exercise 6: Merge with different column names**\n",
        "\n",
        "```python\n",
        "df1 = pd.DataFrame({\"emp_id\": [1,2,3], \"name\": [\"A\",\"B\",\"C\"]})\n",
        "df2 = pd.DataFrame({\"id\": [1,2,4], \"dept\": [\"HR\",\"IT\",\"Sales\"]})\n",
        "```\n",
        "\n",
        "✅ **Solution:**\n",
        "\n",
        "```python\n",
        "pd.merge(df1, df2, left_on=\"emp_id\", right_on=\"id\", how=\"inner\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Exercise 7: Join using index**\n",
        "\n",
        "```python\n",
        "df1 = pd.DataFrame({\"A\": [10,20,30]}, index=[\"x\",\"y\",\"z\"])\n",
        "df2 = pd.DataFrame({\"B\": [40,50,60]}, index=[\"x\",\"y\",\"w\"])\n",
        "```\n",
        "\n",
        "✅ **Solution:**\n",
        "\n",
        "```python\n",
        "df1.join(df2, how=\"outer\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Exercise 8: Concatenation with keys**\n",
        "\n",
        "Combine `df1` and `df2` with a hierarchical index.\n",
        "\n",
        "✅ **Solution:**\n",
        "\n",
        "```python\n",
        "pd.concat([df1, df2], keys=[\"First\", \"Second\"])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Exercise 9: Merge with indicator**\n",
        "\n",
        "See which rows matched or not.\n",
        "\n",
        "✅ **Solution:**\n",
        "\n",
        "```python\n",
        "pd.merge(employees, salaries, on=\"id\", how=\"outer\", indicator=True)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Exercise 10: Sorting after merge**\n",
        "\n",
        "Merge employees and salaries, then sort by salary descending.\n",
        "\n",
        "✅ **Solution:**\n",
        "\n",
        "```python\n",
        "merged = pd.merge(employees, salaries, on=\"id\", how=\"inner\")\n",
        "merged.sort_values(\"salary\", ascending=False)\n",
        "```"
      ],
      "metadata": {
        "id": "ig5iBg9lRVLk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8mPbnEsXRclr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Working with Dates & Time in Pandas\n",
        "\n",
        "Pandas makes it very easy to work with dates, times, and time-series data. It provides flexible tools for parsing, manipulating, and analyzing temporal data.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Converting to DateTime\n",
        "\n",
        "Most datasets have dates stored as **strings** (e.g., `\"2024-08-25\"`) or numbers. We can convert them into Pandas `datetime` objects for powerful operations.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Example\n",
        "df = pd.DataFrame({\n",
        "    \"date_str\": [\"2023-01-01\", \"2023-03-15\", \"2023-07-20\"],\n",
        "    \"sales\": [200, 340, 560]\n",
        "})\n",
        "\n",
        "# Convert string to datetime\n",
        "df[\"date\"] = pd.to_datetime(df[\"date_str\"])\n",
        "print(df)\n",
        "```\n",
        "\n",
        "👉 `pd.to_datetime` automatically detects formats, but you can also specify `format=\"%Y-%m-%d\"` for efficiency.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Extracting Date/Time Components\n",
        "\n",
        "Once you have a `datetime` column, you can extract components:\n",
        "\n",
        "```python\n",
        "df[\"year\"] = df[\"date\"].dt.year\n",
        "df[\"month\"] = df[\"date\"].dt.month\n",
        "df[\"day\"] = df[\"date\"].dt.day\n",
        "df[\"weekday\"] = df[\"date\"].dt.day_name()\n",
        "```\n",
        "\n",
        "👉 This is super useful for grouping (e.g., sales by month, transactions by weekday).\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Creating Date Ranges\n",
        "\n",
        "Generate sequences of dates easily:\n",
        "\n",
        "```python\n",
        "dates = pd.date_range(start=\"2023-01-01\", end=\"2023-01-10\", freq=\"D\")\n",
        "print(dates)\n",
        "```\n",
        "\n",
        "Common frequencies:\n",
        "\n",
        "* `\"D\"` → daily\n",
        "* `\"W\"` → weekly\n",
        "* `\"M\"` → month end\n",
        "* `\"Q\"` → quarter end\n",
        "* `\"Y\"` → year end\n",
        "* `\"H\"` → hourly\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Setting Date as Index\n",
        "\n",
        "Time series analysis often requires setting a datetime column as the index.\n",
        "\n",
        "```python\n",
        "df = df.set_index(\"date\")\n",
        "print(df)\n",
        "```\n",
        "\n",
        "This enables **time-based indexing**:\n",
        "\n",
        "```python\n",
        "df[\"2023-01\"]     # All rows from January 2023\n",
        "df[\"2023-03-15\"]  # Specific date\n",
        "df[\"2023\":\"2023-06\"]  # Slice by range\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Resampling\n",
        "\n",
        "Resampling means changing the frequency of time series data:\n",
        "\n",
        "* **Downsampling**: daily → monthly\n",
        "* **Upsampling**: monthly → daily\n",
        "\n",
        "```python\n",
        "# Example: Resample monthly and take sum of sales\n",
        "monthly_sales = df[\"sales\"].resample(\"M\").sum()\n",
        "print(monthly_sales)\n",
        "```\n",
        "\n",
        "Common resampling methods:\n",
        "\n",
        "* `.sum()` → total\n",
        "* `.mean()` → average\n",
        "* `.count()` → number of entries\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Shifting & Lagging\n",
        "\n",
        "Shift values forward/backward for lag analysis:\n",
        "\n",
        "```python\n",
        "df[\"prev_day_sales\"] = df[\"sales\"].shift(1)\n",
        "df[\"sales_change\"] = df[\"sales\"] - df[\"prev_day_sales\"]\n",
        "```\n",
        "\n",
        "👉 Useful in finance, forecasting, or trend detection.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Rolling Windows\n",
        "\n",
        "Rolling aggregates (moving average, rolling sum, etc.):\n",
        "\n",
        "```python\n",
        "df[\"7d_avg\"] = df[\"sales\"].rolling(window=7).mean()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 💡 Tips & Tricks\n",
        "\n",
        "1. Always use `pd.to_datetime()` before working with dates.\n",
        "2. If your dataset is huge, **specify the format** in `to_datetime` to speed things up.\n",
        "3. Use `.dt` accessor to extract date/time components.\n",
        "4. For time-based grouping, resampling is often easier than `groupby`.\n",
        "5. Use `.asfreq(\"D\")` to reindex with specific frequency and fill missing dates.\n",
        "6. Combine `.shift()` + `.rolling()` for advanced time-series features.\n",
        "\n",
        "---\n",
        "\n",
        "# 📝 10 Exercises (with Solutions)\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create sample dataset\n",
        "dates = pd.date_range(start=\"2023-01-01\", periods=10, freq=\"D\")\n",
        "df = pd.DataFrame({\n",
        "    \"date\": dates,\n",
        "    \"sales\": [200, 220, 250, 180, 300, 270, 290, 310, 400, 420]\n",
        "})\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Convert \"date\" column to datetime\n",
        "\n",
        "✅ Solution:\n",
        "\n",
        "```python\n",
        "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Extract year, month, and weekday\n",
        "\n",
        "✅ Solution:\n",
        "\n",
        "```python\n",
        "df[\"year\"] = df[\"date\"].dt.year\n",
        "df[\"month\"] = df[\"date\"].dt.month\n",
        "df[\"weekday\"] = df[\"date\"].dt.day_name()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Set \"date\" as index\n",
        "\n",
        "✅ Solution:\n",
        "\n",
        "```python\n",
        "df = df.set_index(\"date\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Select sales for `\"2023-01-05\"`\n",
        "\n",
        "✅ Solution:\n",
        "\n",
        "```python\n",
        "df.loc[\"2023-01-05\", \"sales\"]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 5. Select sales between `\"2023-01-03\"` and `\"2023-01-07\"`\n",
        "\n",
        "✅ Solution:\n",
        "\n",
        "```python\n",
        "df.loc[\"2023-01-03\":\"2023-01-07\", \"sales\"]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 6. Resample to weekly sales (sum)\n",
        "\n",
        "✅ Solution:\n",
        "\n",
        "```python\n",
        "df[\"sales\"].resample(\"W\").sum()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 7. Calculate daily difference in sales\n",
        "\n",
        "✅ Solution:\n",
        "\n",
        "```python\n",
        "df[\"daily_change\"] = df[\"sales\"].diff()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 8. Create a 3-day rolling average\n",
        "\n",
        "✅ Solution:\n",
        "\n",
        "```python\n",
        "df[\"3d_avg\"] = df[\"sales\"].rolling(3).mean()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 9. Shift sales by 2 days (lag feature)\n",
        "\n",
        "✅ Solution:\n",
        "\n",
        "```python\n",
        "df[\"lag_2\"] = df[\"sales\"].shift(2)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 10. Fill missing values in resampled daily data\n",
        "\n",
        "✅ Solution:\n",
        "\n",
        "```python\n",
        "df_daily = df[\"sales\"].resample(\"D\").asfreq()\n",
        "df_daily_filled = df_daily.fillna(method=\"ffill\")  # forward fill\n",
        "```\n"
      ],
      "metadata": {
        "id": "49ScjXkkRc66"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sOLwJxuBSNsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **10. Input/Output with Files**\n",
        "\n",
        "Pandas makes it super easy to **read and write** data in different formats.\n",
        "\n",
        "---\n",
        "\n",
        "## **📥 Reading Data**\n",
        "\n",
        "### 1. `pd.read_csv()` → Read CSV file\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Reading CSV file\n",
        "df = pd.read_csv(\"data.csv\")\n",
        "print(df.head())\n",
        "```\n",
        "\n",
        "**Illustration:**\n",
        "If `data.csv` looks like this:\n",
        "\n",
        "```\n",
        "name,age,city\n",
        "Ali,25,Cairo\n",
        "Sara,30,Alexandria\n",
        "```\n",
        "\n",
        "Output:\n",
        "\n",
        "```\n",
        "    name  age       city\n",
        "0    Ali   25      Cairo\n",
        "1   Sara   30  Alexandria\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 2. `pd.read_excel()` → Read Excel file\n",
        "\n",
        "```python\n",
        "df = pd.read_excel(\"data.xlsx\", sheet_name=\"Sheet1\")\n",
        "print(df.head())\n",
        "```\n",
        "\n",
        "Reads directly from Excel sheets 📊.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. `pd.read_json()` → Read JSON file\n",
        "\n",
        "```python\n",
        "df = pd.read_json(\"data.json\")\n",
        "print(df)\n",
        "```\n",
        "\n",
        "**If `data.json` looks like:**\n",
        "\n",
        "```json\n",
        "[\n",
        "  {\"name\": \"Ali\", \"age\": 25},\n",
        "  {\"name\": \"Sara\", \"age\": 30}\n",
        "]\n",
        "```\n",
        "\n",
        "Output:\n",
        "\n",
        "```\n",
        "    name  age\n",
        "0    Ali   25\n",
        "1   Sara   30\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 4. `pd.read_sql()` → Read from SQL Database\n",
        "\n",
        "```python\n",
        "import sqlite3\n",
        "\n",
        "# Connect to database\n",
        "conn = sqlite3.connect(\"mydb.db\")\n",
        "\n",
        "# Read SQL query into DataFrame\n",
        "df = pd.read_sql(\"SELECT * FROM employees\", conn)\n",
        "print(df.head())\n",
        "```\n",
        "\n",
        "Works like pulling data from a database straight into Pandas. 🗄️\n",
        "\n",
        "---\n",
        "\n",
        "## **📤 Writing Data**\n",
        "\n",
        "### 1. `to_csv()` → Save to CSV\n",
        "\n",
        "```python\n",
        "df.to_csv(\"output.csv\", index=False)\n",
        "```\n",
        "\n",
        "💡 `index=False` avoids saving the row numbers.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. `to_excel()` → Save to Excel\n",
        "\n",
        "```python\n",
        "df.to_excel(\"output.xlsx\", index=False, sheet_name=\"Sheet1\")\n",
        "```\n",
        "\n",
        "Great for exporting clean Excel reports. ✅\n",
        "\n",
        "---\n",
        "\n",
        "### 3. `to_json()` → Save to JSON\n",
        "\n",
        "```python\n",
        "df.to_json(\"output.json\", orient=\"records\", lines=True)\n",
        "```\n",
        "\n",
        "💡 `orient=\"records\"` makes JSON more human-readable.\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "## **💡 Tips & Tricks**\n",
        "\n",
        "* Use `nrows` in `pd.read_csv()` to read only part of a large file:\n",
        "\n",
        "  ```python\n",
        "  df = pd.read_csv(\"data.csv\", nrows=100)\n",
        "  ```\n",
        "* Use `chunksize` to read huge files in pieces:\n",
        "\n",
        "  ```python\n",
        "  for chunk in pd.read_csv(\"bigfile.csv\", chunksize=5000):\n",
        "      print(chunk.shape)\n",
        "  ```\n",
        "* For Excel, install **openpyxl** for `.xlsx` files.\n",
        "\n",
        "---\n",
        "\n",
        "## **📝 Exercises**\n",
        "\n",
        "1. Read a CSV file named `students.csv` into a DataFrame.\n",
        "2. Read only the first 50 rows of a CSV file.\n",
        "3. Read the `Sheet2` from an Excel file.\n",
        "4. Read a JSON file named `products.json`.\n",
        "5. Read data from a SQL table named `orders`.\n",
        "6. Save a DataFrame `df` to `result.csv` without the index column.\n",
        "7. Save a DataFrame to Excel with sheet name `Report`.\n",
        "8. Save a DataFrame to JSON in `records` orientation.\n",
        "9. Load a huge CSV file in chunks of 10,000 rows.\n",
        "10. Practice writing the same DataFrame to all 3 formats: CSV, Excel, JSON.\n"
      ],
      "metadata": {
        "id": "Vz_UamaGSOBj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "awOgO2TzUiW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Input/Output with Files in Pandas\n",
        "\n",
        "Pandas makes it super easy to **read from** and **write to** different file formats like CSV, Excel, JSON, SQL databases, and more.\n",
        "\n",
        "---\n",
        "\n",
        "## 📥 Reading Files\n",
        "\n",
        "### 1. `pd.read_csv()` – Read CSV files\n",
        "\n",
        "CSV (Comma-Separated Values) is the most common format.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Reading CSV\n",
        "df = pd.read_csv(\"data.csv\")\n",
        "print(df.head())\n",
        "```\n",
        "\n",
        "🔹 **Tips:**\n",
        "\n",
        "* Use `sep=\";\"` if your file uses semicolons.\n",
        "* Use `nrows=10` to read only the first 10 rows.\n",
        "* Use `usecols=['col1','col2']` to read selected columns.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. `pd.read_excel()` – Read Excel files\n",
        "\n",
        "```python\n",
        "# Reading Excel (requires openpyxl installed)\n",
        "df = pd.read_excel(\"data.xlsx\", sheet_name=\"Sheet1\")\n",
        "print(df.head())\n",
        "```\n",
        "\n",
        "🔹 **Tips:**\n",
        "\n",
        "* `sheet_name=None` → loads all sheets into a dictionary of DataFrames.\n",
        "* `usecols=\"A:C\"` → selects columns A to C.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. `pd.read_json()` – Read JSON files\n",
        "\n",
        "```python\n",
        "# Reading JSON\n",
        "df = pd.read_json(\"data.json\")\n",
        "print(df.head())\n",
        "```\n",
        "\n",
        "🔹 **Tips:**\n",
        "\n",
        "* JSON is often used in APIs.\n",
        "* If the JSON is nested, use `json_normalize` to flatten.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. `pd.read_sql()` – Read from SQL Databases\n",
        "\n",
        "```python\n",
        "import sqlite3\n",
        "\n",
        "# Connect to database\n",
        "conn = sqlite3.connect(\"mydata.db\")\n",
        "\n",
        "# Read SQL query into DataFrame\n",
        "df = pd.read_sql(\"SELECT * FROM employees\", conn)\n",
        "print(df.head())\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 📤 Writing Files\n",
        "\n",
        "### 1. `to_csv()` – Save DataFrame to CSV\n",
        "\n",
        "```python\n",
        "df.to_csv(\"output.csv\", index=False)\n",
        "```\n",
        "\n",
        "🔹 **Tips:**\n",
        "\n",
        "* `index=False` removes the row index column.\n",
        "* `sep=\";\"` → saves with semicolon instead of comma.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. `to_excel()` – Save DataFrame to Excel\n",
        "\n",
        "```python\n",
        "df.to_excel(\"output.xlsx\", sheet_name=\"Results\", index=False)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 3. `to_json()` – Save DataFrame to JSON\n",
        "\n",
        "```python\n",
        "df.to_json(\"output.json\", orient=\"records\", lines=True)\n",
        "```\n",
        "\n",
        "🔹 **Tips:**\n",
        "\n",
        "* `orient=\"records\"` → saves each row as a dictionary.\n",
        "* `lines=True` → saves in newline-delimited JSON (good for big files).\n",
        "\n",
        "---\n",
        "\n",
        "## 💡 Extra Tricks\n",
        "\n",
        "* Use `chunksize=1000` in `read_csv` to read large files in small pieces.\n",
        "* Use compression:\n",
        "\n",
        "  ```python\n",
        "  df.to_csv(\"output.csv.gz\", index=False, compression=\"gzip\")\n",
        "  ```\n",
        "* For very large data, consider **Parquet** format (`read_parquet` / `to_parquet`) for faster performance.\n",
        "\n",
        "---\n",
        "\n",
        "## 📝 Exercises\n",
        "\n",
        "### Exercise 1:\n",
        "\n",
        "Read a CSV file `students.csv` and display only the first 5 rows.\n",
        "\n",
        "### Exercise 2:\n",
        "\n",
        "Read an Excel file `grades.xlsx` from sheet `Math` and display only the columns `Name` and `Score`.\n",
        "\n",
        "### Exercise 3:\n",
        "\n",
        "Save a DataFrame `df` into a CSV file without the index column.\n",
        "\n",
        "### Exercise 4:\n",
        "\n",
        "Convert a DataFrame `df` into JSON format with `orient=\"records\"`.\n",
        "\n",
        "### Exercise 5:\n",
        "\n",
        "Connect to a SQLite database `school.db`, read the table `teachers`, and print the first 3 rows.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Solutions\n",
        "\n",
        "```python\n",
        "# Exercise 1\n",
        "df = pd.read_csv(\"students.csv\")\n",
        "print(df.head())\n",
        "\n",
        "# Exercise 2\n",
        "df = pd.read_excel(\"grades.xlsx\", sheet_name=\"Math\", usecols=[\"Name\", \"Score\"])\n",
        "print(df.head())\n",
        "\n",
        "# Exercise 3\n",
        "df.to_csv(\"students_output.csv\", index=False)\n",
        "\n",
        "# Exercise 4\n",
        "df.to_json(\"students.json\", orient=\"records\")\n",
        "\n",
        "# Exercise 5\n",
        "import sqlite3\n",
        "conn = sqlite3.connect(\"school.db\")\n",
        "df = pd.read_sql(\"SELECT * FROM teachers\", conn)\n",
        "print(df.head(3))\n",
        "```\n"
      ],
      "metadata": {
        "id": "nS-ZhPSSUjAM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-5Uu1nQGUq20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. Advanced Topics in Pandas\n",
        "\n",
        "---\n",
        "\n",
        "## 1. MultiIndex (Hierarchical Indexing)\n",
        "\n",
        "👉 A **MultiIndex** allows you to have multiple levels of indexing (rows or columns). It’s useful for handling higher-dimensional data in a 2D table.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Create a MultiIndex DataFrame\n",
        "arrays = [\n",
        "    ['A', 'A', 'B', 'B'],\n",
        "    ['one', 'two', 'one', 'two']\n",
        "]\n",
        "index = pd.MultiIndex.from_arrays(arrays, names=('letter', 'number'))\n",
        "\n",
        "df = pd.DataFrame({'value': [10, 20, 30, 40]}, index=index)\n",
        "print(df)\n",
        "```\n",
        "\n",
        "**Output:**\n",
        "\n",
        "```\n",
        "             value\n",
        "letter number      \n",
        "A      one       10\n",
        "       two       20\n",
        "B      one       30\n",
        "       two       40\n",
        "```\n",
        "\n",
        "🔹 Accessing data with MultiIndex:\n",
        "\n",
        "```python\n",
        "df.loc['A']           # All rows under 'A'\n",
        "df.loc[('B', 'two')]  # Specific cell\n",
        "```\n",
        "\n",
        "💡 **Tip**: Use `.xs()` (cross section) to slice one level:\n",
        "\n",
        "```python\n",
        "df.xs('one', level='number')\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Window Functions (Rolling, Expanding)\n",
        "\n",
        "👉 Useful for **time-series analysis** (like moving averages).\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "s = pd.Series([1, 2, 3, 4, 5])\n",
        "\n",
        "# Rolling window (moving average)\n",
        "print(s.rolling(window=3).mean())\n",
        "\n",
        "# Expanding window (cumulative mean)\n",
        "print(s.expanding().mean())\n",
        "```\n",
        "\n",
        "**Output (rolling mean):**\n",
        "\n",
        "```\n",
        "0    NaN\n",
        "1    NaN\n",
        "2    2.0\n",
        "3    3.0\n",
        "4    4.0\n",
        "dtype: float64\n",
        "```\n",
        "\n",
        "💡 **Tip**: Combine with `.plot()` to visualize trends smoothly.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Categorical Data\n",
        "\n",
        "👉 Converting columns into **categories** saves memory and speeds up operations.\n",
        "\n",
        "```python\n",
        "df = pd.DataFrame({'fruit': ['apple', 'banana', 'apple', 'orange', 'banana']})\n",
        "\n",
        "# Convert to categorical\n",
        "df['fruit'] = df['fruit'].astype('category')\n",
        "\n",
        "print(df['fruit'].cat.categories)\n",
        "print(df['fruit'].cat.codes)\n",
        "```\n",
        "\n",
        "**Output:**\n",
        "\n",
        "```\n",
        "Index(['apple', 'banana', 'orange'], dtype='object')\n",
        "0    0\n",
        "1    1\n",
        "2    0\n",
        "3    2\n",
        "4    1\n",
        "dtype: int8\n",
        "```\n",
        "\n",
        "💡 **Tip**: Use categorical for columns with **few unique values** (like gender, country codes).\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Sparse Data\n",
        "\n",
        "👉 Sparse data structures help save memory when you have **lots of zeros** or missing values.\n",
        "\n",
        "```python\n",
        "arr = pd.Series([0, 0, 0, 1, 0, 0, 2])\n",
        "sparse_arr = pd.arrays.SparseArray(arr)\n",
        "print(sparse_arr)\n",
        "```\n",
        "\n",
        "**Output:**\n",
        "\n",
        "```\n",
        "[0, 0, 0, 1, 0, 0, 2]\n",
        "Fill: 0\n",
        "IntIndex\n",
        "Indices: array([3, 6])\n",
        "```\n",
        "\n",
        "💡 **Tip**: Store large matrices with many zeros using sparse arrays.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Performance Tips\n",
        "\n",
        "✅ **Vectorization** (avoid loops!):\n",
        "\n",
        "```python\n",
        "# Slow\n",
        "df['new'] = [x * 2 for x in df['value']]\n",
        "\n",
        "# Fast (vectorized)\n",
        "df['new'] = df['value'] * 2\n",
        "```\n",
        "\n",
        "✅ **.eval() & .query()** for faster calculations:\n",
        "\n",
        "```python\n",
        "df = pd.DataFrame({'a': range(100000), 'b': range(100000)})\n",
        "\n",
        "# Faster than df['a'] + df['b']\n",
        "df.eval('c = a + b', inplace=True)\n",
        "\n",
        "# Filtering\n",
        "result = df.query('a < 5 & b > 2')\n",
        "print(result)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# ✨ Exercises (with Solutions)\n",
        "\n",
        "### Q1. Create a MultiIndex DataFrame for regions (Asia, Europe) and countries, then assign random population values.\n",
        "\n",
        "### Q2. Select all data for Europe only using `.loc`.\n",
        "\n",
        "### Q3. Using `.xs()`, select all countries named \"India\".\n",
        "\n",
        "### Q4. Create a Series of numbers and compute a **7-day rolling mean**.\n",
        "\n",
        "### Q5. Compute the expanding mean for the same series.\n",
        "\n",
        "### Q6. Convert a column `[\"yes\", \"no\", \"yes\", \"maybe\"]` to categorical and print codes.\n",
        "\n",
        "### Q7. Create a Series with `[0, 0, 0, 5, 0, 0, 10]` as a sparse array.\n",
        "\n",
        "### Q8. Show the memory difference between categorical and string columns.\n",
        "\n",
        "### Q9. Use `.eval()` to compute `total = col1 + col2` on a DataFrame with 2 numeric columns.\n",
        "\n",
        "### Q10. Use `.query()` to filter a DataFrame where `age > 30 and salary < 5000`.\n",
        "\n",
        "---\n",
        "\n",
        "# ✅ Solutions\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Q1\n",
        "arrays = [['Asia','Asia','Europe','Europe'],['India','China','Germany','France']]\n",
        "index = pd.MultiIndex.from_arrays(arrays, names=('Continent','Country'))\n",
        "df = pd.DataFrame({'Population': [1400, 1300, 80, 65]}, index=index)\n",
        "\n",
        "# Q2\n",
        "print(df.loc['Europe'])\n",
        "\n",
        "# Q3\n",
        "print(df.xs('India', level='Country'))\n",
        "\n",
        "# Q4\n",
        "s = pd.Series(np.arange(1,15))\n",
        "print(s.rolling(window=7).mean())\n",
        "\n",
        "# Q5\n",
        "print(s.expanding().mean())\n",
        "\n",
        "# Q6\n",
        "c = pd.Series([\"yes\",\"no\",\"yes\",\"maybe\"], dtype=\"category\")\n",
        "print(c.cat.codes)\n",
        "\n",
        "# Q7\n",
        "sparse_series = pd.arrays.SparseArray([0,0,0,5,0,0,10])\n",
        "print(sparse_series)\n",
        "\n",
        "# Q8\n",
        "df2 = pd.DataFrame({\"Answer\":[\"yes\",\"no\",\"yes\",\"maybe\"]*1000})\n",
        "print(df2.memory_usage(deep=True))\n",
        "df2['Answer'] = df2['Answer'].astype('category')\n",
        "print(df2.memory_usage(deep=True))\n",
        "\n",
        "# Q9\n",
        "df3 = pd.DataFrame({\"col1\": [1,2,3], \"col2\": [4,5,6]})\n",
        "df3.eval(\"total = col1 + col2\", inplace=True)\n",
        "print(df3)\n",
        "\n",
        "# Q10\n",
        "df4 = pd.DataFrame({\"age\":[25,35,40], \"salary\":[4000,6000,3000]})\n",
        "print(df4.query(\"age > 30 and salary < 5000\"))\n",
        "```"
      ],
      "metadata": {
        "id": "NCR5KYFWUrVs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GE-UmmH2Vv19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project 1: Titanic Dataset Analysis\n",
        "\n",
        "### Problem Statement\n",
        "\n",
        "Analyze the Titanic dataset to uncover insights about survival patterns.\n",
        "\n",
        "**Tasks:**\n",
        "\n",
        "1. Download and load the dataset.\n",
        "2. Clean missing values in `\"Age\"` and `\"Embarked\"`.\n",
        "3. Compute survival rates by `\"Sex\"` and `\"Pclass\"`.\n",
        "4. Create a pivot table showing survival rate by `\"Pclass\"` and `\"Sex\"`.\n",
        "5. Visualize survival rates using a bar chart.\n",
        "\n",
        "### Download the Data\n",
        "\n",
        "You can download the CSV directly:\n",
        "\n",
        "```\n",
        "https://calmcode.io/static/data/titanic.csv\n",
        "```\n",
        "\n",
        "([CalmCode][1])\n",
        "\n",
        "### Solution Code\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Load dataset\n",
        "url = \"https://calmcode.io/static/data/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# 2. Clean missing values\n",
        "df[\"Age\"].fillna(df[\"Age\"].median(), inplace=True)\n",
        "df[\"Embarked\"].fillna(df[\"Embarked\"].mode()[0], inplace=True)\n",
        "\n",
        "# 3. Survival rates by Sex and Pclass\n",
        "survival_by_sex = df.groupby(\"Sex\")[\"Survived\"].mean()\n",
        "survival_by_pclass = df.groupby(\"Pclass\")[\"Survived\"].mean()\n",
        "\n",
        "# 4. Pivot table\n",
        "pivot = df.pivot_table(\n",
        "    values=\"Survived\",\n",
        "    index=\"Pclass\",\n",
        "    columns=\"Sex\",\n",
        "    aggfunc=\"mean\"\n",
        ")\n",
        "\n",
        "# 5. Visualization\n",
        "pivot.plot(kind=\"bar\", figsize=(8,6))\n",
        "plt.title(\"Survival Rate by Class and Sex\")\n",
        "plt.ylabel(\"Survival Rate\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.legend(title=\"Sex\")\n",
        "plt.show()\n",
        "\n",
        "# Display results\n",
        "print(\"Survival Rate by Sex:\\n\", survival_by_sex)\n",
        "print(\"\\nSurvival Rate by Pclass:\\n\", survival_by_pclass)\n",
        "print(\"\\nPivot Table:\\n\", pivot)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Project 2: Retail Sales Dataset Analysis\n",
        "\n",
        "### Problem Statement\n",
        "\n",
        "Examine a retail sales dataset to understand trends and product performance.\n",
        "\n",
        "**Tasks:**\n",
        "\n",
        "1. Download and load the dataset.\n",
        "2. Convert `\"Date\"` to datetime and clean duplicates.\n",
        "3. Compute a new `\"Total\"` column (`Quantity * Price`).\n",
        "4. Group sales by `\"Region\"` and `\"Product Category\"` to find total and average sales.\n",
        "5. Identify the overall best-selling product.\n",
        "6. Plot total sales by region.\n",
        "\n",
        "### Download the Data\n",
        "\n",
        "Use this sample dataset (1,000 rows, retail sales details):\n",
        "\n",
        "```\n",
        "Sample Retail Sales Dataset (1000 rows, 10 columns including date, product category, quantity, price, region)\n",
        "```\n",
        "\n",
        "([Gigasheet][2])\n",
        "*(You’ll need to copy the data manually or use your preferred CSV source.)*\n",
        "\n",
        "### Solution Code\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Load dataset\n",
        "# Example path:\n",
        "# df = pd.read_csv(\"retail-sales.csv\")\n",
        "# For demonstration, we'll create a simulated dataset:\n",
        "\n",
        "import numpy as np\n",
        "date_range = pd.date_range(start=\"2025-01-01\", periods=100, freq=\"D\")\n",
        "np.random.seed(0)\n",
        "df = pd.DataFrame({\n",
        "    \"Date\": np.random.choice(date_range, 1000),\n",
        "    \"Product Category\": np.random.choice([\"Electronics\", \"Clothing\", \"Groceries\"], 1000),\n",
        "    \"Quantity\": np.random.randint(1, 5, 1000),\n",
        "    \"Price\": np.random.uniform(5.0, 100.0, 1000),\n",
        "    \"Region\": np.random.choice([\"North\", \"South\", \"East\", \"West\"], 1000)\n",
        "})\n",
        "\n",
        "# 2. Clean data\n",
        "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "# 3. Compute Total\n",
        "df[\"Total\"] = df[\"Quantity\"] * df[\"Price\"]\n",
        "\n",
        "# 4. Group by Region & Product Category\n",
        "grouped = df.groupby([\"Region\", \"Product Category\"]).agg(\n",
        "    Total_Sales=(\"Total\", \"sum\"),\n",
        "    Avg_Sales=(\"Total\", \"mean\"),\n",
        "    Count=(\"Total\", \"count\")\n",
        ").reset_index()\n",
        "\n",
        "# 5. Best-selling product\n",
        "best_product = grouped.loc[grouped[\"Total_Sales\"].idxmax()]\n",
        "\n",
        "# 6. Plot total sales by region\n",
        "region_sales = df.groupby(\"Region\")[\"Total\"].sum()\n",
        "region_sales.plot(kind=\"bar\", figsize=(8,6))\n",
        "plt.title(\"Total Sales by Region\")\n",
        "plt.ylabel(\"Total Sales\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.show()\n",
        "\n",
        "# Display results\n",
        "print(\"Sales by Region & Product:\\n\", grouped.head())\n",
        "print(\"\\nBest-selling product entry:\\n\", best_product)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Czjkj0ltWq_8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lbiydlgqWs31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Project 3: Movies Dataset – Ratings Analysis**\n",
        "\n",
        "**Problem Statement:**\n",
        "The Movies dataset contains movie titles, genres, ratings, and release years. Your task is to clean the dataset, find the most popular genres, and analyze average ratings per year.\n",
        "\n",
        "**Dataset:** [IMDB Movies Dataset](https://www.kaggle.com/datasets/ashirwadsangwan/imdb-dataset)\n",
        "\n",
        "**Steps & Solution:**\n",
        "\n",
        "```python\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"imdb_top_1000.csv\")\n",
        "\n",
        "# Most common genres\n",
        "genres = df['Genre'].str.split(',').explode().value_counts().head(5)\n",
        "print(\"Top 5 Genres:\\n\", genres)\n",
        "\n",
        "# Average rating per year\n",
        "avg_rating_year = df.groupby('Year')['IMDB Rating'].mean()\n",
        "print(\"Average Rating by Year:\\n\", avg_rating_year.head())\n",
        "\n",
        "# Highest rated movie per genre\n",
        "best_per_genre = df.groupby('Genre')['IMDB Rating'].max()\n",
        "print(\"Best Movies per Genre:\\n\", best_per_genre)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **Project 4: Weather Data Analysis**\n",
        "\n",
        "**Problem Statement:**\n",
        "You have a weather dataset containing temperature, humidity, and wind speed recorded daily. Your task is to analyze temperature trends, detect missing values, and compute monthly averages.\n",
        "\n",
        "**Dataset:** [Daily Weather Dataset](https://www.kaggle.com/datasets/muthuj7/weather-dataset)\n",
        "\n",
        "**Steps & Solution:**\n",
        "\n",
        "```python\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"DailyDelhiClimateTrain.csv\")\n",
        "\n",
        "# Convert date column\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "# Missing values\n",
        "print(\"Missing Values:\\n\", df.isna().sum())\n",
        "\n",
        "# Monthly average temperature\n",
        "monthly_temp = df.resample('M', on='date')['meantemp'].mean()\n",
        "print(\"Monthly Avg Temperature:\\n\", monthly_temp)\n",
        "\n",
        "# Highest temp day\n",
        "hottest_day = df.loc[df['meantemp'].idxmax()]\n",
        "print(\"Hottest Day:\\n\", hottest_day)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "TkSF9TgeWtTT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TTvkEYb3eYut"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}